{
  "metadata": {
    "kernelspec": {
      "name": "py-dku-containerized-venv-env_forecasting-eks-default",
      "display_name": "Python in eks-default (env env_forecasting)",
      "language": "python"
    },
    "associatedRecipe": "compute_metrics",
    "dkuGit": {
      "lastInteraction": 0
    },
    "creationTag": {
      "versionNumber": 0,
      "lastModifiedBy": {
        "login": "Subham.Sarkar"
      },
      "lastModifiedOn": 1651590026288
    },
    "creator": "Subham.Sarkar",
    "createdOn": 1651590026288,
    "tags": [
      "recipe-editor"
    ],
    "customFields": {},
    "hide_input": false,
    "language_info": {
      "name": "python",
      "version": "3.6.8",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "modifiedBy": "Subham.Sarkar"
  },
  "nbformat": 4,
  "nbformat_minor": 1,
  "cells": [
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# -*- coding: utf-8 -*-\nimport dataiku\nimport pandas as pd, numpy as np\nfrom dataiku import pandasutils as pdu"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport itertools\n# import tensorflow as tf\nimport pandas as pd\nimport numpy as np\nfrom datetime import date\nimport os\nimport time\n\n# from tqdm.notebook import tqdm\n# tqdm.pandas()\nfrom tqdm import tqdm\n#%matplotlib inline\nfrom sklearn.metrics import mean_squared_error\nfrom functools import reduce\n\n### Create the Stacked LSTM model\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense,LSTM,GRU,Dropout\nfrom sklearn.preprocessing import MinMaxScaler\nfrom pmdarima.arima import auto_arima\nimport xgboost as xgb\nfrom sklearn.model_selection import RandomizedSearchCV\nimport datetime\nfrom dateutil.relativedelta import relativedelta\nimport holidays\nfrom collections import defaultdict\nimport statsmodels.api as sm\n#pd.set_option(\u0027display.max_columns\u0027, None)\n#pd.set_option(\u0027display.max_rows\u0027, 100)\n\nfrom helper_mp import find_len,state_length_check,data_transform_to_id_level,\\\ndates_checker,dates_checker_2,padder,mean_absolute_percentage_error,create_dataset,create_lstm,\\\nfit_model,create_and_train_data_for_hw,create_and_train_data_for_arimax,\\\ncreate_and_train_data_for_xgboost,compare_models,choose_model_and_forecast,\\\nthree_months_mape_calc,weighted_mape_calc,efd_feature_generator,merge_EFD_feature_file,\\\ncreate_and_train_data_for_lstm_multivariate,records_maintainer,metric_file_generator,\\\nmerge_cardcounts_feature_file,\\\nmetric_file_generator_for_exception_accounts,merge_CMD_feature_file,twelve_months_mape_calc,\\\ntwelve_months_rmse_calc,mean_calc,generate_mobility_feature,merge_weighted_fuel_price_feature_file"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import multiprocessing as mp\nfrom concurrent.futures import ProcessPoolExecutor\nimport gc\nfrom joblib import Parallel, delayed"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "## Main holidays\nholidays_me \u003d pd.DataFrame()\nfor yr in range(2008, 2025, 1):\n    me_cal \u003d pd.DataFrame(holidays.US(state\u003d\u0027ME\u0027, years\u003dyr).items(), columns \u003d [\u0027REV_CALENDAR_DATE\u0027, \u0027HOLIDAY_NAME\u0027])\n    me_cal[\u0027REV_CALENDAR_DATE\u0027] \u003d pd.to_datetime(me_cal[\u0027REV_CALENDAR_DATE\u0027])\n    me_cal[\u0027HOLIDAY\u0027] \u003d 1\n    holidays_me \u003d pd.concat([holidays_me, me_cal])"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "path_params \u003d eval(dataiku.get_custom_variables()[\u0027path_params\u0027])\nrevenue_date_EFD_by_day_path \u003d path_params[\"revenue_date_EFD_by_day_path\"]\ndaily_gallons_full_path \u003d path_params[\"daily_gallons_full_path\"]\n\nmultivariate_lstm_params \u003d eval(dataiku.get_custom_variables()[\u0027multivariate_lstm_params\u0027])\ntrain_period_lstm_model \u003d multivariate_lstm_params[\"train_period_lstm_model\"]\npred_period_lstm_model \u003d multivariate_lstm_params[\"pred_period_lstm_model\"]\nnum_units_lstm_model \u003d multivariate_lstm_params[\"num_units_lstm_model\"]\noptimizer_lstm_model \u003d multivariate_lstm_params[\"optimizer_lstm_model\"]\nloss_function_lstm_model \u003d multivariate_lstm_params[\"loss_function_lstm_model\"]\nbatch_size_lstm_model \u003d multivariate_lstm_params[\"batch_size_lstm_model\"]\nnum_epochs_lstm_model \u003d multivariate_lstm_params[\"num_epochs_lstm_model\"]\n\nhw_params \u003d eval(dataiku.get_custom_variables()[\u0027hw_params\u0027])\nseasonal_period \u003d hw_params[\"seasonal_period\"]\ntrend \u003d hw_params[\"trend\"]\nseasonal_hw \u003d hw_params[\"seasonal\"]\nalpha \u003d hw_params[\"alpha\"]\nbeta \u003d hw_params[\"beta\"]\ngamma \u003d hw_params[\"gamma\"]\n\narimax_params \u003d eval(dataiku.get_custom_variables()[\u0027arimax_params\u0027])\ninformation_criterion \u003d arimax_params[\"information_criterion\"]\nseasonal_arimax \u003d arimax_params[\"seasonal\"]\nn_jobs \u003d arimax_params[\"n_jobs\"]\n\nxgboost_params \u003d eval(dataiku.get_custom_variables()[\u0027xgboost_params\u0027])\nlearning_rate_xgb \u003d xgboost_params[\"learning_rate\"]\nn_estimators_xgb \u003d xgboost_params[\"n_estimators\"]\nsubsample_xgb \u003d xgboost_params[\"subsample\"]\nmax_depth_xgb \u003d xgboost_params[\"max_depth\"]\ncolsample_bytree_xgb \u003d xgboost_params[\"colsample_bytree\"]\nmin_child_weight_xgb \u003d xgboost_params[\"min_child_weight\"]\n\nprimary_params \u003d eval(dataiku.get_custom_variables()[\u0027primary_params\u0027])\ndate_year \u003d primary_params[\"date_year\"]\ndate_month \u003d primary_params[\"date_month\"]\nforecast_target_column \u003d primary_params[\"forecast_target_column\"]\nbusiness_program_name \u003d primary_params[\"business_program_name\"]\n\nglobal_params \u003d eval(dataiku.get_custom_variables()[\u0027global_params\u0027])\nmodels \u003d global_params[\"models\"]\nmodel_for_exception_accounts \u003d global_params[\"model_for_exception_accounts\"]\ngranularity_level \u003d global_params[\"granularity_level\"]\ncurrent_month \u003d global_params[\"current_month\"]\nvalidation_window_length \u003d global_params[\"evaluation_window_length\"]\nfuture_prediction_months \u003d global_params[\"future_prediction_months\"]\nperformance_assessment_window \u003d global_params[\"performance_assessment_window\"]\nexternal_feature_list \u003d global_params[\"external_feature_list\"]\nchoice \u003d global_params[\"choice\"]\n\nefd_lstm_params \u003d eval(dataiku.get_custom_variables()[\u0027efd_lstm_params\u0027])\nlstm_train_window_length_EFD \u003d efd_lstm_params[\"lstm_train_window_length\"]\nnum_units_EFD \u003d efd_lstm_params[\"num_units\"]\nactivation_function_EFD \u003d efd_lstm_params[\"activation_function\"]\noptimizer_EFD \u003d efd_lstm_params[\"optimizer\"]\nloss_function_EFD \u003d efd_lstm_params[\"loss_function\"]\nbatch_size_EFD \u003d efd_lstm_params[\"batch_size\"]\nnum_epochs_EFD \u003d efd_lstm_params[\"num_epochs\"]\nMODELS_EFD \u003d efd_lstm_params[\"MODELS\"]\nma_mapping_EFD \u003d efd_lstm_params[\"ma_mapping\"]"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_pd \u003d dataiku.Dataset(\"volume_gallons_by_account_sanitized\").get_dataframe()\n\nDATE \u003d []\nfor y, m in zip(dataset_pd[date_year], dataset_pd[date_month]):\n    DATE.append(date(y, m, 1))\ndataset_pd[\u0027revenue_date\u0027] \u003d DATE\n\n## data pre processing\ndataset_pd[\u0027revenue_date\u0027] \u003d pd.to_datetime(dataset_pd[\u0027revenue_date\u0027])\ndataset_pd.sort_values([\u0027revenue_date\u0027], inplace\u003dTrue)\n\ndataset_pd[\u0027location_state\u0027] \u003d dataset_pd[\u0027location_state\u0027].str.upper()\n\npadd1A_states \u003d [\u0027CT\u0027,\u0027ME\u0027,\u0027MA\u0027,\u0027NH\u0027,\u0027RI\u0027,\u0027VT\u0027]\npadd1B_states \u003d [\u0027DE\u0027,\u0027DC\u0027,\u0027MD\u0027,\u0027NJ\u0027,\u0027NY\u0027,\u0027PA\u0027]\npadd1C_states \u003d [\u0027FL\u0027,\u0027GA\u0027,\u0027NC\u0027,\u0027SC\u0027,\u0027VA\u0027,\u0027WV\u0027]\npadd2_states \u003d [\u0027IL\u0027,\u0027IN\u0027,\u0027IA\u0027,\u0027KS\u0027,\u0027KY\u0027,\u0027MI\u0027,\u0027MN\u0027,\u0027MO\u0027,\u0027NE\u0027,\u0027ND\u0027,\u0027OH\u0027,\u0027OK\u0027,\u0027SD\u0027,\u0027TN\u0027,\u0027WI\u0027]\npadd3_states \u003d [\u0027AL\u0027,\u0027AR\u0027,\u0027LA\u0027,\u0027MS\u0027,\u0027NM\u0027,\u0027TX\u0027]\npadd4_states \u003d [\u0027CO\u0027,\u0027ID\u0027,\u0027MT\u0027,\u0027UT\u0027,\u0027WY\u0027]\npadd5_states \u003d [\u0027AK\u0027,\u0027AZ\u0027,\u0027CA\u0027,\u0027HI\u0027,\u0027NV\u0027,\u0027OR\u0027,\u0027WA\u0027]\n\nconditions \u003d [(dataset_pd[\u0027location_state\u0027].isin(padd1A_states)) | (dataset_pd[\u0027location_state\u0027].isin(padd1B_states)) | (dataset_pd[\u0027location_state\u0027].isin(padd1C_states)) ,\n              dataset_pd[\u0027location_state\u0027].isin(padd2_states),\n              dataset_pd[\u0027location_state\u0027].isin(padd3_states),\n              dataset_pd[\u0027location_state\u0027].isin(padd4_states),\n              dataset_pd[\u0027location_state\u0027].isin(padd5_states)]\nchoices \u003d    [\u0027PADD1\u0027,\n              \u0027PADD2\u0027,\n              \u0027PADD3\u0027,\n              \u0027PADD4\u0027,\n              \u0027PADD5\u0027]\ndataset_pd[\u0027PADD_regions\u0027] \u003d np.select(conditions, choices, default\u003d\u0027unknown\u0027)\n\ndataset_pd \u003d dataset_pd[dataset_pd[\u0027PADD_regions\u0027]!\u003d\u0027unknown\u0027]\n\ndataset_pd.rename(columns \u003d {\u0027global_customer_id\u0027:granularity_level},inplace\u003dTrue)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "grpd_by_regions_gallons_sum \u003d dataset_pd.groupby([granularity_level,\u0027revenue_date\u0027,\u0027PADD_regions\u0027]).agg(gallons_sum \u003d (\u0027purchase_gallons_qty\u0027,sum))\ngrpd_by_regions_gallons_sum.reset_index(inplace\u003dTrue)\ngrpd_by_regions_gallons_sum \u003d grpd_by_regions_gallons_sum.groupby([granularity_level,\u0027revenue_date\u0027]).agg(PADD_regions \u003d (\u0027PADD_regions\u0027,list),gallons_sum \u003d (\u0027gallons_sum\u0027,list))\ngrpd_by_regions_gallons_sum.reset_index(inplace\u003dTrue)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "mdm \u003d dataiku.Dataset(\"SANITIZEDATAFOROFFSHORE.mdm_final_sanitized\").get_dataframe(columns\u003d[\u0027accountnumber\u0027,\u0027businessprogramname\u0027,granularity_level,\u0027location_state\u0027])\n#Fill state\nid_to_state \u003d dict(zip(mdm[granularity_level],mdm[\u0027location_state\u0027]))"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Writing code to project gallons at padd level"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "grpd_by_regions_gallons_sum \u003d grpd_by_regions_gallons_sum[(pd.to_datetime(grpd_by_regions_gallons_sum[\u0027revenue_date\u0027])\u003cpd.to_datetime(pd.to_datetime(current_month)+pd.DateOffset(months\u003d-(validation_window_length-1))))]"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "grpd_by_regions_gallons_sum_exploded \u003d grpd_by_regions_gallons_sum.apply(pd.Series.explode)\n\ngrpd_by_regions_gallons_sum_required \u003d grpd_by_regions_gallons_sum_exploded.groupby([granularity_level,\u0027PADD_regions\u0027]).agg(rev_date \u003d (\u0027revenue_date\u0027,list),gallons_list \u003d (\u0027gallons_sum\u0027,list))\ngrpd_by_regions_gallons_sum_required.reset_index(inplace\u003dTrue)\ngrpd_by_regions_gallons_sum_required[\u0027total_records_before_padding\u0027] \u003d grpd_by_regions_gallons_sum_required.apply(find_len,axis\u003d1)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "current_month_efd \u003d str(pd.to_datetime(current_month) + pd.DateOffset(months\u003d-validation_window_length)).split()[0]\nvalidation_window_length_efd \u003d validation_window_length*2\nexternal_feature_list_PADD \u003d [\u0027lag_12\u0027,\u0027lag_9\u0027,\u0027lag_6\u0027,\u0027lag_3\u0027,\u0027lag_2\u0027,\u0027lag_1\u0027,\u0027MA12\u0027,\u0027MA6\u0027,\u0027MSD12\u0027,\u0027MSD6\u0027,\u0027REV_EQUIVALENT_FUEL_DAY_FACTOR_efd_ma3m\u0027,\u0027covid_mobility_feature\u0027]"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "efd_features_monthly \u003d efd_feature_generator(holidays_me,current_month_efd,lstm_train_window_length_EFD,num_units_EFD,activation_function_EFD,optimizer_EFD,loss_function_EFD,batch_size_EFD,num_epochs_EFD,MODELS_EFD,ma_mapping_EFD,revenue_date_EFD_by_day_path,daily_gallons_full_path,validation_window_length_efd,future_prediction_months)\n\nefd_features_monthly\u003defd_features_monthly.reset_index()\n\nefd_features_monthly \u003d efd_features_monthly.rename(columns \u003d {\u0027index\u0027:\u0027rev_date\u0027})\n\nefd_features_monthly[\u0027rev_date\u0027] \u003d pd.to_datetime(efd_features_monthly[\u0027rev_date\u0027])"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "covid_mobility_df \u003d generate_mobility_feature(current_month \u003d current_month_efd, future_prediction_months \u003d validation_window_length_efd)\n\ncovid_mobility_df \u003d covid_mobility_df.rename(columns\u003d{\u0027month\u0027:\u0027rev_date\u0027})\ncovid_mobility_df \u003d covid_mobility_df.groupby(\u0027rev_date\u0027).agg(retail_and_recreation_percent_change_from_baseline \u003d (\u0027retail_and_recreation_percent_change_from_baseline\u0027,sum))\ncovid_mobility_df.reset_index(inplace\u003dTrue)\ncovid_mobility_df \u003d covid_mobility_df[covid_mobility_df[\u0027rev_date\u0027]\u003c\u003d(pd.to_datetime(current_month)+ pd.DateOffset(months \u003d validation_window_length))]"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def processing_engine_for_gallons_forecast(dataset_pd,grpd_by_regions_gallons_sum_required,current_month_efd,validation_window_length_efd,external_feature_list_PADD):\n    start_time \u003d time.time()\n\n    # Import and transform data to the desired level (ID-month level or business_program_name-month level)\n    grouped_id_level \u003d grpd_by_regions_gallons_sum_required.copy()\n    print(grouped_id_level.shape)\n\n    def dates_checker_gallons(df_dt,current_month):#function designed due to lack of historical data before 2019,else use date_checker2\n        row \u003d df_dt[\u0027rev_date\u0027]\n        years_list \u003d set(list(map(lambda x:x.year,row)))\n        first_date \u003d row[0]\n        last_date \u003d row[-1]\n        if first_date \u003c\u003d (current_month + pd.DateOffset(months\u003d-24+1)) and current_month.year -1 in(years_list) and current_month.year in(years_list): #and last_date \u003e pd.to_datetime(current_month):\n            return 1\n        elif first_date \u003e (current_month + pd.DateOffset(months\u003d-24+1)) and current_month.year in(years_list): #and last_date \u003e pd.to_datetime(current_month):\n            return 2\n        else:\n            return 0\n\n    # Account should have atleast 2 years of data to train check\n    grouped_id_level[\u0027year_check\u0027]  \u003d grouped_id_level.apply(dates_checker_gallons,current_month\u003dpd.to_datetime(current_month_efd),axis\u003d1)\n    grouped_id_level \u003d grouped_id_level[grouped_id_level[\u0027year_check\u0027]\u003d\u003d1]\n    print(grouped_id_level.shape)\n#     grouped_id_level\u003dgrouped_id_level.head(30)\n\n    # Missing Value treatment\n    combined_date_list \u003d []\n    combined_gallons_list \u003d []\n    ans \u003d grouped_id_level.apply(padder,\n                                  combined_date_list\u003dcombined_date_list,\n                                  combined_gallons_list\u003dcombined_gallons_list,\n                                  validation_window_length\u003dvalidation_window_length,\n                                  current_month\u003dpd.to_datetime(current_month),axis\u003d1)\n    grouped_id_level[\u0027rev_date\u0027] \u003d combined_date_list\n    grouped_id_level[\u0027gallons_list\u0027] \u003d combined_gallons_list\n    grouped_id_level[\u0027total_records_after_padding\u0027] \u003d grouped_id_level.apply(find_len,axis\u003d1)\n    grouped_id_level_ind \u003d grouped_id_level.reset_index()\n\n    # Adding EFD as a feature\n    grouped_id_level_ind[\u0027REV_EQUIVALENT_FUEL_DAY_FACTOR_efd\u0027],\\\n    grouped_id_level_ind[\u0027REV_EQUIVALENT_FUEL_DAY_FACTOR_efd_ma3m\u0027],\\\n    grouped_id_level_ind[\u0027REV_EQUIVALENT_FUEL_DAY_FACTOR_efd_ma6m\u0027]\u003d\\\n    zip(*grouped_id_level_ind.apply(merge_EFD_feature_file,\n                                     efd_features_monthly\u003defd_features_monthly,\n                                     axis\u003d1))\n\n    # Adding covid mobility as a feature\n    grouped_id_level_ind[\u0027covid_mobility_feature\u0027]\u003dgrouped_id_level_ind.apply(merge_CMD_feature_file,\n                                                                               cmd_features_monthly\u003dcovid_mobility_df,\n                                                                               axis\u003d1)\n\n    print(\"--- %s seconds for data preparation---\" % (time.time() - start_time))\n    # Building models from the multiple options available\n    start_time \u003d time.time()\n\n    models_trained \u003d []\n\n    if \u0027HW\u0027 in models:\n        print(\u0027Running HW\u0027)\n        rows \u003d [(grouped_id_level_ind.loc[i],\n                 pd.to_datetime(current_month_efd),\n                 validation_window_length_efd,\n                 alpha,\n                 beta,\n                 gamma,\n                 granularity_level) for i in grouped_id_level_ind.index]\n        results_hw_list \u003d Parallel(n_jobs\u003dmp.cpu_count())(delayed(create_and_train_data_for_hw)(a,b,c,d,e,f,g) for a,b,c,d,e,f,g in rows)\n\n        results_hw \u003d pd.DataFrame(results_hw_list, columns\u003d[granularity_level, \u0027mape_hw\u0027, \u0027rmse_hw\u0027,\\\n                            \u0027agorithm_used_hw\u0027, \u0027rev_date_hw\u0027, \u0027ground_truth_hw\u0027, \u0027predictions_hw\u0027])\n        results_hw[\u0027agorithm_used_hw\u0027] \u003d \u0027HW\u0027\n        results_hw[[granularity_level,\u0027PADD_regions\u0027]]\u003dresults_hw[granularity_level].str.split(\u0027_\u0027,expand\u003dTrue)\n        results_hw[granularity_level] \u003d results_hw[granularity_level].astype(\u0027int64\u0027)\n        models_trained.append(results_hw)\n        del rows\n        gc.collect()\n\n    if \u0027XGB\u0027 in models and len(external_feature_list)\u003e0:\n        print(\u0027Running XGB\u0027)\n        rows \u003d [(grouped_id_level_ind.loc[i],\n                pd.to_datetime(current_month_efd),\n                validation_window_length_efd,\n                external_feature_list_PADD,\n                learning_rate_xgb,\n                n_estimators_xgb,\n                subsample_xgb,\n                max_depth_xgb,\n                colsample_bytree_xgb,\n                min_child_weight_xgb,\n                granularity_level) for i in grouped_id_level_ind.index]\n        results_xgb_list \u003d Parallel(n_jobs\u003dmp.cpu_count())(delayed(create_and_train_data_for_xgboost)(a,b,c,d,e,f,g,h,i,j,k) for a,b,c,d,e,f,g,h,i,j,k in rows)\n\n        results_xgb \u003d pd.DataFrame(results_xgb_list, columns\u003d[granularity_level, \u0027mape_xgb\u0027, \u0027rmse_xgb\u0027,\n                            \u0027agorithm_used_xgb\u0027, \u0027rev_date_xgb\u0027, \u0027ground_truth_xgb\u0027, \u0027predictions_xgb\u0027])\n        results_xgb[\u0027agorithm_used_xgb\u0027] \u003d \u0027XGB\u0027\n        results_xgb[[granularity_level,\u0027PADD_regions\u0027]]\u003dresults_xgb[granularity_level].str.split(\u0027_\u0027,expand\u003dTrue)\n        results_xgb[granularity_level] \u003d results_xgb[granularity_level].astype(\u0027int64\u0027)\n        models_trained.append(results_xgb)\n        del rows\n        gc.collect()\n    if \u0027ARIMAX\u0027 in models and len(external_feature_list)\u003e0:\n        print(\u0027Running ARIMAX\u0027)\n        rows \u003d [(grouped_id_level_ind.loc[i],\n                 pd.to_datetime(current_month_efd),\n                 validation_window_length_efd,\n                 information_criterion,\n                 seasonal_arimax,\n                 external_feature_list_PADD,\n                 n_jobs,\n                 granularity_level) for i in grouped_id_level_ind.index]\n        results_arimax_list \u003d Parallel(n_jobs\u003dmp.cpu_count())(delayed(create_and_train_data_for_arimax)(a,b,c,d,e,f,g,h) for a,b,c,d,e,f,g,h in rows)\n        results_arimax \u003d pd.DataFrame(results_arimax_list, columns\u003d[\n            granularity_level, \u0027mape_arimax\u0027, \u0027rmse_arimax\u0027, \u0027agorithm_used_arimax\u0027,\n            \u0027rev_date_arimax\u0027, \u0027ground_truth_arimax\u0027, \u0027predictions_arimax\u0027\n        ])\n        results_arimax[\u0027agorithm_used_arimax\u0027] \u003d \u0027ARIMAX\u0027\n        results_arimax[[granularity_level,\u0027PADD_regions\u0027]]\u003dresults_arimax[granularity_level].str.split(\u0027_\u0027,expand\u003dTrue)\n        results_arimax[granularity_level] \u003d results_arimax[granularity_level].astype(\u0027int64\u0027)\n        models_trained.append(results_arimax)\n        del rows\n        gc.collect()\n\n\n    if \u0027LSTM_Multivariate\u0027 in models and len(external_feature_list)\u003e0:\n        print(\u0027Running LSTM_Multivariate\u0027)\n        rows \u003d [(grouped_id_level_ind.loc[i],\n                 pd.to_datetime(current_month_efd),\n                 validation_window_length_efd,\n                 external_feature_list_PADD,\n                 train_period_lstm_model,\n                 pred_period_lstm_model,\n                 optimizer_lstm_model,\n                 loss_function_lstm_model,\n                 batch_size_lstm_model,\n                 num_epochs_lstm_model,\n                granularity_level) for i in grouped_id_level_ind.index]\n        results_lstm_list \u003d Parallel(n_jobs\u003dmp.cpu_count())(delayed(create_and_train_data_for_lstm_multivariate)(a,b,c,d,e,f,g,h,i,j,k) for a,b,c,d,e,f,g,h,i,j,k in rows)\n\n        results_lstm \u003d pd.DataFrame(results_lstm_list, columns\u003d[\n            granularity_level, \u0027mape_lstm_multivariate\u0027, \u0027rmse_lstm_multivariate\u0027,\n            \u0027agorithm_used_lstm_multivariate\u0027,\n            \u0027rev_date_lstm_multivariate\u0027, \u0027ground_truth_lstm_multivariate\u0027,\n            \u0027predictions_lstm_multivariate\u0027\n        ])\n        results_lstm[\u0027agorithm_used_lstm_multivariate\u0027] \u003d \u0027LSTM_Multivariate\u0027\n        results_lstm[[granularity_level,\u0027PADD_regions\u0027]]\u003dresults_lstm[granularity_level].str.split(\u0027_\u0027,expand\u003dTrue)\n        results_lstm[granularity_level] \u003d results_lstm[granularity_level].astype(\u0027int64\u0027)\n\n        models_trained.append(results_lstm)\n        del rows\n        gc.collect()\n\n\n    print(\"--- %s seconds for building models---\" % (time.time() - start_time))\n\n    ## COMPARE MODELS\n    print(\u0027Initialising Compare models\u0027)\n    start_time \u003d time.time()\n    id_best_algo_df \u003d compare_models(models_trained,granularity_level)\n    grouped_id_level_ind \u003d grouped_id_level_ind.merge(id_best_algo_df,on\u003dgranularity_level,how\u003d\u0027left\u0027)\n    print(\"--- %s seconds for comparing models---\" % (time.time() - start_time))\n\n    ## CHOOSE and FORECAST using the best model\n    print(\u0027Initialising Forecasting \u0027)\n    start_time \u003d time.time()\n\n    rows \u003d [(grouped_id_level_ind.loc[i],\n            pd.to_datetime(current_month_efd),\n            validation_window_length_efd,\n            information_criterion,\n            seasonal_arimax,\n            external_feature_list_PADD,\n            n_jobs,\n            learning_rate_xgb,\n            n_estimators_xgb,\n            subsample_xgb,\n            max_depth_xgb,\n            colsample_bytree_xgb,\n            min_child_weight_xgb,\n            train_period_lstm_model,\n            pred_period_lstm_model,\n            optimizer_lstm_model,\n            loss_function_lstm_model,\n            batch_size_lstm_model,\n            num_epochs_lstm_model,\n            alpha,\n            beta,\n            gamma,\n            granularity_level) for i in grouped_id_level_ind.index]\n    final_res_list \u003d Parallel(n_jobs\u003dmp.cpu_count())(delayed(choose_model_and_forecast)(a,b,c,d,e,f,g,h,i,j,k,l,m,n,o,p,q,r,s,t,u,v,w) for a,b,c,d,e,f,g,h,i,j,k,l,m,n,o,p,q,r,s,t,u,v,w in rows)\n\n    final_res \u003d pd.DataFrame(final_res_list, columns\u003d[granularity_level,\n                                                     \u0027overall_mape(performance_assessment_period)\u0027,\n                                                     \u0027overall_rmse(performance_assessment_period)\u0027,\n                                                     \u0027model_chosen\u0027,\n                                                     \u0027month\u0027,\n                                                     \u0027gallons_actual(performance_assessment_period)\u0027,\n                                                     \u0027gallons_forecasts(performance_assessment_period)\u0027])\n    final_res[[granularity_level,\u0027PADD_regions\u0027]]\u003dfinal_res[granularity_level].str.split(\u0027_\u0027,expand\u003dTrue)\n    final_res[granularity_level] \u003d final_res[granularity_level].astype(\u0027int64\u0027)\n    del rows\n    gc.collect()\n    error_file \u003d final_res[final_res[\u0027model_chosen\u0027]\u003d\u003d\u0027ERROR\u0027]\n    final_res \u003d final_res[final_res[\u0027model_chosen\u0027]!\u003d\u0027ERROR\u0027]\n    final_res.reset_index(drop\u003dTrue,inplace\u003dTrue)\n\n    #final_res[\u0027wex_id\u0027],final_res[\u0027performance_assesment_mape\u0027],final_res[\u0027performance_assesment_rmse\u0027],final_res[\u0027agorithm_used\u0027],final_res[\u0027performance_assesment_rev_date\u0027],final_res[\u0027performance_assesment_ground_truth\u0027],final_res[\u0027performance_assesment_forecasts\u0027],final_res[\u002724_months_forecasts\u0027]\u003d zip(*grouped_wex_id_level_ind.apply(choose_model_and_forecast,future_prediction_months\u003dfuture_prediction_months,current_month\u003dcurrent_month,performance_assessment_window\u003dperformance_assessment_window,trend\u003dtrend,seasonal_hw\u003dseasonal_hw,seasonal_periods\u003dseasonal_period,information_criterion\u003dinformation_criterion,seasonal_arimax\u003dseasonal_arimax,external_feature_list\u003dexternal_feature_list,n_jobs\u003dn_jobs,learning_rate_xgb\u003dlearning_rate_xgb,n_estimators_xgb\u003dn_estimators_xgb,subsample_xgb\u003dsubsample_xgb,max_depth_xgb\u003dmax_depth_xgb,colsample_bytree_xgb\u003dcolsample_bytree_xgb,min_child_weight_xgb\u003dmin_child_weight_xgb,axis\u003d1))\n    print(\"--- %s seconds for forecasting using best model---\" % (time.time() - start_time))\n\n    ## Output the required files\n    start_time \u003d time.time()\n    final_res[\u00273_month_mape(performance_assesment_period)\u0027] \u003d final_res.apply(three_months_mape_calc,axis\u003d1)\n    final_res[\u0027time_weighted_mape(performance_assesment_period)\u0027] \u003d final_res.apply(weighted_mape_calc,axis\u003d1)\n\n    final_res[\u0027overall_mape(performance_assessment_period)\u0027] \u003d final_res[\u0027overall_mape(performance_assessment_period)\u0027].astype(\u0027float64\u0027)\n    final_res[\u0027time_weighted_mape(performance_assesment_period)\u0027] \u003d final_res[\u0027time_weighted_mape(performance_assesment_period)\u0027].astype(\u0027float64\u0027)\n    final_res[\u00273_month_mape(performance_assesment_period)\u0027] \u003d final_res[\u00273_month_mape(performance_assesment_period)\u0027].astype(\u0027float64\u0027)\n\n    final_res[\u0027overall_mape(performance_assessment_period)\u0027] \u003d round(final_res[\u0027overall_mape(performance_assessment_period)\u0027],2)\n    final_res[\u0027time_weighted_mape(performance_assesment_period)\u0027] \u003d round(final_res[\u0027time_weighted_mape(performance_assesment_period)\u0027],2)\n    final_res[\u00273_month_mape(performance_assesment_period)\u0027] \u003d round(final_res[\u00273_month_mape(performance_assesment_period)\u0027],2)\n    df_dict \u003d dict()\n    for idx,dfs in enumerate(models_trained):\n        model \u003d dfs.iloc[0,3]\n        df_dict[model] \u003d idx\n\n    final_res[\u0027overall_mape(evaluation_period)\u0027],final_res[\u00273_month_mape(evaluation_period)\u0027],\\\n    final_res[\u0027time_weighted_mape(evaluation_period)\u0027],final_res[\u0027monthly_avg_volume(evaluation_period)\u0027] \u003d\\\n    zip(*final_res.apply(records_maintainer,\n                                  df_dict\u003ddf_dict,models_trained\u003dmodels_trained,\n                                  granularity_level\u003dgranularity_level,axis\u003d1))\n\n    metrics_file \u003d final_res[[granularity_level,\u0027PADD_regions\u0027,\u0027model_chosen\u0027,\u0027monthly_avg_volume(evaluation_period)\u0027,\u0027overall_mape(evaluation_period)\u0027,\n                             \u00273_month_mape(evaluation_period)\u0027,\u0027time_weighted_mape(evaluation_period)\u0027,\n                             \u0027overall_mape(performance_assessment_period)\u0027,\u00273_month_mape(performance_assesment_period)\u0027,\n                             \u0027time_weighted_mape(performance_assesment_period)\u0027]]\n\n\n    final_res \u003d final_res.merge(grouped_id_level_ind[[granularity_level,\u0027PADD_regions\u0027,\u0027rev_date\u0027,\u0027gallons_list\u0027]], on \u003d[granularity_level,\u0027PADD_regions\u0027],how\u003d\u0027left\u0027)\n    metric_file \u003d pd.DataFrame()\n    metric_file[\u0027data\u0027] \u003d final_res.apply(metric_file_generator,\n                                                   df_dict\u003ddf_dict, models_trained\u003dmodels_trained,models\u003dmodels,\n                                                   granularity_level\u003dgranularity_level,axis\u003d1)\n    total_forecast_file \u003d pd.DataFrame()\n    for i in range(len(metric_file)):\n        total_forecast_file \u003d total_forecast_file.append(metric_file.iloc[i][0],ignore_index\u003dTrue)\n\n    ##Exception accounts\n    print(\u0027Handling Exception accounts\u0027)\n    final_res_B \u003d final_res[final_res[\u0027overall_mape(evaluation_period)\u0027]\u003e\u003d40]\n    final_res11gt40_wids \u003d list(final_res_B[granularity_level])\n\n    grouped_id_level \u003d grpd_by_regions_gallons_sum_required.copy()\n\n    # Account should have atleast 2 years of data to train check\n    grouped_id_level[\u0027year_check\u0027]  \u003d grouped_id_level.apply(dates_checker_gallons,current_month\u003dpd.to_datetime(current_month_efd),axis\u003d1)\n\n    grouped_id_level_ind_A \u003d grouped_id_level.reset_index()\n\n    grouped_id_level_ind_A[\u0027ev_mape_gt40_check\u0027]  \u003d np.where((grouped_id_level_ind_A[granularity_level].isin(final_res11gt40_wids)),1,0)\n    grouped_id_level \u003d grouped_id_level_ind_A.set_index(granularity_level)\n\n    grouped_id_level_AB \u003d grouped_id_level[(grouped_id_level[\u0027year_check\u0027]\u003d\u003d2) | (grouped_id_level[\u0027ev_mape_gt40_check\u0027]\u003d\u003d1)]\n\n    #Padding\n    combined_date_list_AB \u003d []\n    combined_gallons_list_AB \u003d []\n    ans_AB \u003d grouped_id_level_AB.apply(padder,\n                                        combined_date_list\u003dcombined_date_list_AB,\n                                        combined_gallons_list\u003dcombined_gallons_list_AB,\n                                        validation_window_length\u003dvalidation_window_length,\n                                        current_month\u003dpd.to_datetime(current_month),axis\u003d1)\n    grouped_id_level_AB[\u0027rev_date\u0027] \u003d combined_date_list_AB\n    grouped_id_level_AB[\u0027gallons_list\u0027] \u003d combined_gallons_list_AB\n    grouped_id_level_AB[\u0027total_records_after_padding\u0027] \u003d grouped_id_level_AB.apply(find_len,axis\u003d1)\n    grouped_id_level_ind_AB \u003d grouped_id_level_AB.reset_index()\n#     grouped_id_level_ind_AB[\u0027state\u0027] \u003d grouped_id_level_ind_AB.apply(state_length_check,axis\u003d1)\n    grouped_id_level_ind_AB \u003d grouped_id_level_ind_AB[grouped_id_level_ind_AB[\u0027total_records_after_padding\u0027]\u003e\u003d18]\n\n    ##storing ids\n    smallaccounts_ids \u003d list(grouped_id_level_ind_AB[grouped_id_level_ind_AB[\u0027year_check\u0027]\u003d\u003d2][granularity_level])\n\n    grouped_id_level_ind_AB[\u0027REV_EQUIVALENT_FUEL_DAY_FACTOR_efd\u0027],\\\n    grouped_id_level_ind_AB[\u0027REV_EQUIVALENT_FUEL_DAY_FACTOR_efd_ma3m\u0027],\\\n    grouped_id_level_ind_AB[\u0027REV_EQUIVALENT_FUEL_DAY_FACTOR_efd_ma6m\u0027]\u003d\\\n    zip(*grouped_id_level_ind_AB.apply(merge_EFD_feature_file,efd_features_monthly\u003defd_features_monthly,axis\u003d1))\n\n    # Adding covid mobility as a feature\n    grouped_id_level_ind_AB[\u0027covid_mobility_feature\u0027]\u003dgrouped_id_level_ind_AB.apply(merge_CMD_feature_file,\n                                                                                       cmd_features_monthly\u003dcovid_mobility_df,\n                                                                                       axis\u003d1)\n\n    ## Modelling for Exception accounts\n#     grouped_id_level_ind_AB \u003d grouped_id_level_ind_AB.sample(10)\n    #ARIMAX\n    grouped_id_level_ind_AB \u003d grouped_id_level_ind_AB.assign(best_algo\u003d\u0027ARIMAX\u0027)   #model_for_exception_accounts\n    rows \u003d [(grouped_id_level_ind_AB.loc[i],\n            pd.to_datetime(current_month_efd),\n            validation_window_length_efd,\n            information_criterion,\n            seasonal_arimax,\n            external_feature_list_PADD,\n            n_jobs,\n            learning_rate_xgb,\n            n_estimators_xgb,\n            subsample_xgb,\n            max_depth_xgb,\n            colsample_bytree_xgb,\n            min_child_weight_xgb,\n            train_period_lstm_model,\n            pred_period_lstm_model,\n            optimizer_lstm_model,\n            loss_function_lstm_model,\n            batch_size_lstm_model,\n            num_epochs_lstm_model,\n            alpha,\n            beta,\n            gamma,\n            granularity_level) for i in grouped_id_level_ind_AB.index]\n    final_res_exp_list_1 \u003d Parallel(n_jobs\u003dmp.cpu_count())(delayed(choose_model_and_forecast)(a,b,c,d,e,f,g,h,i,j,k,l,m,n,o,p,q,r,s,t,u,v,w) for a,b,c,d,e,f,g,h,i,j,k,l,m,n,o,p,q,r,s,t,u,v,w in rows)\n\n    final_res_exp_1 \u003d pd.DataFrame(final_res_exp_list_1, columns\u003d[granularity_level,\n                                                     \u0027overall_mape(performance_assessment_period)\u0027,\n                                                     \u0027overall_rmse(performance_assessment_period)\u0027,\n                                                     \u0027model_chosen\u0027,\n                                                     \u0027month\u0027,\n                                                     \u0027gallons_actual(performance_assessment_period)\u0027,\n                                                     \u0027gallons_forecasts(performance_assessment_period)\u0027])\n    final_res_exp_1[[granularity_level,\u0027PADD_regions\u0027]]\u003dfinal_res_exp_1[granularity_level].str.split(\u0027_\u0027,expand\u003dTrue)\n    final_res_exp_1[granularity_level] \u003d final_res_exp_1[granularity_level].astype(\u0027int64\u0027)\n    del rows\n    gc.collect()\n\n    #LSTM_MULTIVARIATE\n    grouped_id_level_ind_AB \u003d grouped_id_level_ind_AB.assign(best_algo\u003d\u0027LSTM_MULTIVARIATE\u0027)   #model_for_exception_accounts\n    train_period_lstm_model_exp \u003d 5\n    rows \u003d [(grouped_id_level_ind_AB.loc[i],\n            pd.to_datetime(current_month_efd),\n            validation_window_length_efd,\n            information_criterion,\n            seasonal_arimax,\n            external_feature_list_PADD,\n            n_jobs,\n            learning_rate_xgb,\n            n_estimators_xgb,\n            subsample_xgb,\n            max_depth_xgb,\n            colsample_bytree_xgb,\n            min_child_weight_xgb,\n            train_period_lstm_model_exp,\n            pred_period_lstm_model,\n            optimizer_lstm_model,\n            loss_function_lstm_model,\n            batch_size_lstm_model,\n            num_epochs_lstm_model,\n            alpha,\n            beta,\n            gamma,\n            granularity_level) for i in grouped_id_level_ind_AB.index]\n    final_res_exp_list_2 \u003d Parallel(n_jobs\u003dmp.cpu_count())(delayed(choose_model_and_forecast)(a,b,c,d,e,f,g,h,i,j,k,l,m,n,o,p,q,r,s,t,u,v,w) for a,b,c,d,e,f,g,h,i,j,k,l,m,n,o,p,q,r,s,t,u,v,w in rows)\n\n    final_res_exp_2 \u003d pd.DataFrame(final_res_exp_list_2, columns\u003d[granularity_level,\n                                                     \u0027overall_mape(performance_assessment_period)\u0027,\n                                                     \u0027overall_rmse(performance_assessment_period)\u0027,\n                                                     \u0027model_chosen\u0027,\n                                                     \u0027month\u0027,\n                                                     \u0027gallons_actual(performance_assessment_period)\u0027,\n                                                     \u0027gallons_forecasts(performance_assessment_period)\u0027])\n    final_res_exp_2[[granularity_level,\u0027PADD_regions\u0027]]\u003dfinal_res_exp_2[granularity_level].str.split(\u0027_\u0027,expand\u003dTrue)\n    final_res_exp_2[granularity_level] \u003d final_res_exp_2[granularity_level].astype(\u0027int64\u0027)\n    del rows\n    gc.collect()\n\n    #ENSEMBLER\n    final_res_exp \u003d pd.DataFrame()\n    gallons_forecasts \u003d []\n\n    for i in range(final_res_exp_2.shape[0]):\n        gallons_forecasts.append(mean_calc(final_res_exp_1[\u0027gallons_forecasts(performance_assessment_period)\u0027].iloc[i],final_res_exp_2[\u0027gallons_forecasts(performance_assessment_period)\u0027].iloc[i]))\n    final_res_exp[granularity_level] \u003d final_res_exp_2[granularity_level]\n    final_res_exp[\u0027gallons_actual(performance_assessment_period)\u0027] \u003d final_res_exp_2[\u0027gallons_actual(performance_assessment_period)\u0027]\n    final_res_exp[\u0027gallons_forecasts(performance_assessment_period)\u0027] \u003d gallons_forecasts\n    final_res_exp[\u0027model_chosen\u0027] \u003d \u0027ENSEMBLE(LSTM_and_ARIMAX)\u0027\n    final_res_exp[\u0027month\u0027] \u003d final_res_exp_2[\u0027month\u0027]\n    final_res_exp[\u0027overall_mape(performance_assessment_period)\u0027] \u003d final_res_exp.apply(twelve_months_mape_calc,axis\u003d1)\n    final_res_exp[\u0027overall_rmse(performance_assessment_period)\u0027] \u003d final_res_exp.apply(twelve_months_rmse_calc,axis\u003d1)\n    final_res_exp[\u0027PADD_regions\u0027] \u003d final_res_exp_2[\u0027PADD_regions\u0027]\n\n    error_file_exp \u003d final_res_exp[final_res_exp[\u0027model_chosen\u0027]\u003d\u003d\u0027ERROR\u0027]\n    final_res_exp \u003d final_res_exp[final_res_exp[\u0027model_chosen\u0027]!\u003d\u0027ERROR\u0027]\n    final_res_exp.reset_index(drop\u003dTrue,inplace\u003dTrue)\n\n    final_res_exp[\u00273_month_mape(performance_assesment_period)\u0027] \u003d final_res_exp.apply(three_months_mape_calc,axis\u003d1)\n    final_res_exp[\u0027time_weighted_mape(performance_assesment_period)\u0027] \u003d final_res_exp.apply(weighted_mape_calc,axis\u003d1)\n\n    final_res_exp \u003d final_res_exp.merge(grouped_id_level_ind_AB[[granularity_level,\u0027PADD_regions\u0027,\u0027rev_date\u0027,\u0027gallons_list\u0027]],on\u003d[granularity_level,\u0027PADD_regions\u0027],how\u003d\u0027left\u0027)\n    metrics_fileAB \u003d final_res_exp[[granularity_level,\u0027model_chosen\u0027,\u0027overall_mape(performance_assessment_period)\u0027,\n                              \u00273_month_mape(performance_assesment_period)\u0027,\u0027time_weighted_mape(performance_assesment_period)\u0027]]\n\n    metric_fileAB \u003d pd.DataFrame()\n    metric_fileAB[\u0027data\u0027] \u003d final_res_exp.apply(metric_file_generator_for_exception_accounts,granularity_level\u003dgranularity_level,axis\u003d1)\n    total_forecast_fileAB \u003d pd.DataFrame()\n    for i in range(len(metric_fileAB)):\n        total_forecast_fileAB \u003d total_forecast_fileAB.append(metric_fileAB.iloc[i][0],ignore_index\u003dTrue)\n\n    exception_ids \u003d list(final_res_exp[granularity_level])\n    metrics_file \u003d metrics_file[~(metrics_file[granularity_level].isin(exception_ids))]\n    total_forecast_file \u003d total_forecast_file[~(total_forecast_file[granularity_level].isin(exception_ids))]\n    metrics_file \u003d metrics_file[~(metrics_file[granularity_level].isin(exception_ids))]\n    final_res \u003d final_res[~(final_res[granularity_level].isin(exception_ids))]\n\n    total_forecast_file \u003d total_forecast_file.append(total_forecast_fileAB).reset_index(drop\u003dTrue)\n    metrics_file \u003d metrics_file.append(metrics_fileAB).reset_index(drop\u003dTrue)\n    final_res \u003d final_res.append(final_res_exp).reset_index(drop\u003dTrue)\n\n    ## Putting status  flags to differentiate different accounts\n    conditions \u003d [final_res[granularity_level].isin(final_res11gt40_wids),\n                  final_res[granularity_level].isin(smallaccounts_ids)]\n    choices \u003d    [\u0027accounts with greater than 40 evaluation period MAPE\u0027,\n                  \u0027accounts with less than 36 datapoints\u0027]\n    final_res[\u0027status_flag\u0027] \u003d np.select(conditions, choices, default\u003d\u0027eligible accounts\u0027)\n\n    conditions \u003d [total_forecast_file[granularity_level].isin(final_res11gt40_wids),\n                  total_forecast_file[granularity_level].isin(smallaccounts_ids)]\n    choices \u003d    [\u0027accounts with greater than 40 evaluation period MAPE\u0027,\n                  \u0027accounts with less than 36 datapoints\u0027]\n    total_forecast_file[\u0027status_flag\u0027] \u003d np.select(conditions, choices, default\u003d\u0027eligible accounts\u0027)\n\n    conditions \u003d [metrics_file[granularity_level].isin(final_res11gt40_wids),\n                  metrics_file[granularity_level].isin(smallaccounts_ids)]\n    choices \u003d    [\u0027accounts with greater than 40 evaluation period MAPE\u0027,\n                  \u0027accounts with less than 36 datapoints\u0027]\n    metrics_file[\u0027status_flag\u0027] \u003d np.select(conditions, choices, default\u003d\u0027eligible accounts\u0027)\n\n\n    metrics_file.drop([\u0027overall_mape(performance_assessment_period)\u0027,\n                       \u00273_month_mape(performance_assesment_period)\u0027,\n                       \u0027time_weighted_mape(performance_assesment_period)\u0027],axis\u003d1,inplace\u003dTrue)\n\n\n#     metrics_file.to_csv(\u0027metrics_file.csv\u0027,index\u003dFalse)\n#     total_forecast_file.to_csv(\u0027forecasts.csv\u0027,index\u003dFalse)\n#     final_res.to_csv(\u0027final_results.csv\u0027,index\u003dFalse)\n    print(\"--- %s seconds for outputting the required files---\" % (time.time() - start_time))\n\n\n    return final_res,metrics_file,total_forecast_file"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "final_res_PADD,metrics_file_PADD,total_forecast_file_PADD \u003d processing_engine_for_gallons_forecast(dataset_pd,grpd_by_regions_gallons_sum_required,current_month_efd,validation_window_length_efd,external_feature_list_PADD)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# grpd_by_regions_gallons_sum_required1 \u003d grpd_by_regions_gallons_sum_required.copy()"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# grpd_by_regions_gallons_sum_required \u003d grpd_by_regions_gallons_sum_required1.copy()"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "grpd_by_regions_gallons_sum_required\u003dgrpd_by_regions_gallons_sum_required[grpd_by_regions_gallons_sum_required[granularity_level].isin(final_res_PADD[granularity_level])]\n\n\ngrpd_by_regions_gallons_sum_required \u003d grpd_by_regions_gallons_sum_required.sort_values([granularity_level,\u0027PADD_regions\u0027],ascending\u003d[True,True])\n\ngrpd_by_regions_gallons_sum_required.reset_index(drop\u003dTrue,inplace\u003dTrue)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "final_res_PADD \u003d final_res_PADD.sort_values([granularity_level,\u0027PADD_regions\u0027],ascending\u003d[True,True])\n\nfinal_res_PADD.reset_index(drop\u003dTrue,inplace\u003dTrue)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# grpd_by_regions_gallons_sum_required1 \u003d grpd_by_regions_gallons_sum_required.copy()"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# grpd_by_regions_gallons_sum_required \u003d grpd_by_regions_gallons_sum_required[[granularity_level,\u0027PADD_regions\u0027,\u0027rev_date\u0027,\u0027gallons_list\u0027]].merge(final_res_PADD[[\u0027month\u0027,\u0027gallons_forecasts(performance_assessment_period)\u0027,\u0027model_chosen\u0027]],on\u003dgrpd_by_regions_gallons_sum_required.index,how\u003d\u0027left\u0027)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "grpd_by_regions_gallons_sum_required \u003d final_res_PADD[[granularity_level,\u0027PADD_regions\u0027,\u0027month\u0027,\u0027gallons_forecasts(performance_assessment_period)\u0027,\u0027model_chosen\u0027 ]]"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "error_file_PADD \u003d grpd_by_regions_gallons_sum_required[grpd_by_regions_gallons_sum_required[\u0027model_chosen\u0027].isin([\u0027ERROR\u0027,\u0027ERRORS\u0027])]\ngrpd_by_regions_gallons_sum_required \u003d grpd_by_regions_gallons_sum_required[~(grpd_by_regions_gallons_sum_required[\u0027model_chosen\u0027].isin([\u0027ERROR\u0027,\u0027ERRORS\u0027]))]"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# grpd_by_regions_gallons_sum_required.drop([\u0027key_0\u0027,\u0027rev_date\u0027,\u0027gallons_list\u0027],inplace\u003dTrue,axis\u003d1)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "grpd_by_regions_gallons_sum_required.reset_index(drop\u003dTrue,inplace\u003dTrue)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def explode_df_for_PADD_regions(df,granularity_level):\n\n    wid \u003d df[granularity_level]\n    padd_regions \u003d df[\u0027PADD_regions\u0027]\n\n    overall_df \u003d pd.DataFrame()\n    overall_df[\u0027rev_date\u0027] \u003d df[\u0027month\u0027]\n    overall_df[\u0027forecasts\u0027] \u003d df[\u0027gallons_forecasts(performance_assessment_period)\u0027]\n    overall_df \u003d overall_df.apply(pd.Series.explode)\n    overall_df[granularity_level] \u003d [wid]*len(overall_df)\n    overall_df[\u0027PADD_regions\u0027] \u003d [padd_regions]*len(overall_df)\n    overall_df \u003d overall_df[[granularity_level,\u0027PADD_regions\u0027,\u0027rev_date\u0027,\u0027forecasts\u0027]]\n\n    return overall_df"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "metric_fileAB \u003d pd.DataFrame()\nmetric_fileAB[\u0027data\u0027] \u003d grpd_by_regions_gallons_sum_required.apply(explode_df_for_PADD_regions,granularity_level\u003dgranularity_level,axis\u003d1)\ngrpd_by_regions_gallons_sum_required_exploded \u003d pd.DataFrame()\nfor i in range(len(metric_fileAB)):\n    grpd_by_regions_gallons_sum_required_exploded \u003d grpd_by_regions_gallons_sum_required_exploded.append(metric_fileAB.iloc[i][0],ignore_index\u003dTrue)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "grpd_by_regions_gallons_sum_future \u003d grpd_by_regions_gallons_sum_required_exploded.groupby([granularity_level,\u0027rev_date\u0027]).agg(PADD_regions \u003d (\u0027PADD_regions\u0027,list),gallons_sum \u003d (\u0027forecasts\u0027,list))"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "grpd_by_regions_gallons_sum_future.reset_index(inplace\u003dTrue)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "grpd_by_regions_gallons_sum_future \u003d grpd_by_regions_gallons_sum_future.rename(columns\u003d{\u0027rev_date\u0027:\u0027revenue_date\u0027})"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "grpd_by_regions_gallons_sum \u003d grpd_by_regions_gallons_sum[grpd_by_regions_gallons_sum[granularity_level].isin(grpd_by_regions_gallons_sum_future[granularity_level].unique())]"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "grpd_by_regions_gallons_sum \u003d grpd_by_regions_gallons_sum.append(grpd_by_regions_gallons_sum_future)\ngrpd_by_regions_gallons_sum \u003d grpd_by_regions_gallons_sum.sort_values([granularity_level,\u0027revenue_date\u0027],ascending\u003d[True,True])"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "grpd_by_regions_gallons_sum.reset_index(drop\u003dTrue,inplace\u003dTrue)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## EFD Prediction"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "efd_features_monthly \u003d efd_feature_generator(holidays_me,current_month,lstm_train_window_length_EFD,num_units_EFD,activation_function_EFD,optimizer_EFD,loss_function_EFD,batch_size_EFD,num_epochs_EFD,MODELS_EFD,ma_mapping_EFD,revenue_date_EFD_by_day_path,daily_gallons_full_path,validation_window_length,future_prediction_months)\n\nefd_features_monthly\u003defd_features_monthly.reset_index()\n\nefd_features_monthly \u003d efd_features_monthly.rename(columns \u003d {\u0027index\u0027:\u0027rev_date\u0027})\n\nefd_features_monthly[\u0027rev_date\u0027] \u003d pd.to_datetime(efd_features_monthly[\u0027rev_date\u0027])"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Google mobility feature for covid months"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "covid_mobility_df \u003d generate_mobility_feature(current_month \u003d current_month, future_prediction_months \u003d validation_window_length)\n\ncovid_mobility_df \u003d covid_mobility_df.rename(columns\u003d{\u0027month\u0027:\u0027rev_date\u0027})\ncovid_mobility_df \u003d covid_mobility_df.groupby(\u0027rev_date\u0027).agg(retail_and_recreation_percent_change_from_baseline \u003d (\u0027retail_and_recreation_percent_change_from_baseline\u0027,sum))\ncovid_mobility_df.reset_index(inplace\u003dTrue)\ncovid_mobility_df \u003d covid_mobility_df[covid_mobility_df[\u0027rev_date\u0027]\u003c\u003dpd.to_datetime(current_month)+ pd.DateOffset(months \u003d validation_window_length)]\n# covid_mobility_df.tail()"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Adding fuel prices as feature"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# fuel_price_data \u003d pd.read_csv(\u0027D:/Users/W505723/Downloads/Sanitized_dataset/fule_prices_data_projections.csv\u0027)\nfuel_price_data \u003d dataiku.Dataset(\"fule_prices_data_projections\").get_dataframe()\n\nfuel_price_data \u003d fuel_price_data.T\n\nfuel_price_data.columns \u003d [fuel_price_data.iloc[0]]\nfuel_price_data \u003d fuel_price_data[1:]\nfuel_price_data  \u003d fuel_price_data.iloc[: , :5]\nfuel_price_data.reset_index(inplace\u003dTrue)\nfuel_price_data.columns \u003d fuel_price_data.columns.get_level_values(0)\ndates \u003d pd.date_range(\u00271997-01-01\u0027,\u002712-01-2023\u0027,freq\u003d\u0027MS\u0027)\nfuel_price_data[\u0027rev_date\u0027] \u003d dates\nfuel_price_data.drop([\u0027index\u0027],axis\u003d1,inplace\u003dTrue)\n\n\nfuel_price_data \u003d fuel_price_data.rename(columns\u003d{\u0027PADD 1 (East Coast)\u0027:\u0027PADD_1\u0027,\\\n                                                  \u0027PADD 2 (Midwest)\u0027:\u0027PADD_2\u0027,\\\n                                                  \u0027PADD 3 (Gulf Coast)\u0027:\u0027PADD_3\u0027,\\\n                                                  \u0027PADD 4 (Rocky Mountain)\u0027:\u0027PADD_4\u0027,\\\n                                                  \u0027PADD 5 (West Coast)\u0027:\u0027PADD_5\u0027})\n\nfuel_price_data[[\u0027PADD_1\u0027, \u0027PADD_2\u0027, \u0027PADD_3\u0027, \u0027PADD_4\u0027, \u0027PADD_5\u0027]] \u003d fuel_price_data[[\u0027PADD_1\u0027, \u0027PADD_2\u0027, \u0027PADD_3\u0027, \u0027PADD_4\u0027, \u0027PADD_5\u0027]]/100"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fuel_price_data\u003dfuel_price_data[fuel_price_data[\u0027rev_date\u0027]\u003c\u003d (pd.to_datetime(current_month)+ pd.DateOffset(months \u003d validation_window_length))]"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "grpd_by_regions_gallons_sum \u003d grpd_by_regions_gallons_sum.merge(fuel_price_data,left_on\u003d\u0027revenue_date\u0027,right_on\u003d\u0027rev_date\u0027,how\u003d\u0027left\u0027)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# grpd_by_regions_gallons_sum1 \u003d grpd_by_regions_gallons_sum.copy()"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# grpd_by_regions_gallons_sum \u003d grpd_by_regions_gallons_sum1.copy()"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# def checkIfDuplicates_1(df):\n#     listOfElems \u003d df[\u0027PADD_regions\u0027]\n#     print(listOfElems,len(listOfElems),str(listOfElems),len(str(listOfElems)),\u0027*****\u0027)\n#     \u0027\u0027\u0027 Check if given list contains any duplicates \u0027\u0027\u0027\n#     if len(listOfElems) \u003d\u003d len(set(listOfElems)):\n#         return False\n#     else:\n#         return True\n# grpd_by_regions_gallons_sum[\u0027dups\u0027] \u003d grpd_by_regions_gallons_sum.apply(checkIfDuplicates_1,axis\u003d1)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# grpd_by_regions_gallons_sum \u003d grpd_by_regions_gallons_sum.tail(12000)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\u0027Calculating weighted fuel price\u0027)\ndef weighted_average_fuel_price(df):\n    try:\n        fuel_prices_PADDwise \u003d []\n        fuel_prices_PADDwise.append(df[\u0027PADD_1\u0027])\n        fuel_prices_PADDwise.append(df[\u0027PADD_2\u0027])\n        fuel_prices_PADDwise.append(df[\u0027PADD_3\u0027])\n        fuel_prices_PADDwise.append(df[\u0027PADD_4\u0027])\n        fuel_prices_PADDwise.append(df[\u0027PADD_5\u0027])\n\n        padd_regions_ind \u003d df[\u0027PADD_regions\u0027]\n        padd_regions_gallons \u003d df[\u0027gallons_sum\u0027]\n        df2 \u003d  pd.DataFrame({\u0027padd_regions_gallons\u0027: padd_regions_gallons},index\u003dpadd_regions_ind)\n        if df2.index.is_unique\u003d\u003dFalse:\n            print(False)\n        df2 \u003d df2.groupby(df2.index).sum()\n        padd_regions_new_ind \u003d [\u0027PADD1\u0027, \u0027PADD2\u0027, \u0027PADD3\u0027, \u0027PADD4\u0027, \u0027PADD5\u0027]\n        df2 \u003d df2.reindex(padd_regions_new_ind, fill_value\u003d0)\n        weights_PADDwise \u003d df2[\u0027padd_regions_gallons\u0027]\n\n        weighted_sum \u003d []\n        for price, weights in zip(fuel_prices_PADDwise, weights_PADDwise):\n            weighted_sum.append(price * weights)\n\n        weighted_fuel_price \u003d round(sum(weighted_sum) / (sum(weights_PADDwise)+0.0001),3)\n        weighted_fuel_price_x_gallons \u003d weighted_fuel_price* sum(weights_PADDwise)\n        return weighted_fuel_price,weighted_fuel_price_x_gallons\n    except:\n        print(\u0027ERROR in : \u0027,df[\u0027global_customer_id\u0027])\n        return 0,0\n\n\ngrpd_by_regions_gallons_sum[\u0027weighted_average_fuel_price\u0027],grpd_by_regions_gallons_sum[\u0027weighted_fuel_price_x_gallons\u0027] \u003d zip(*grpd_by_regions_gallons_sum.apply(weighted_average_fuel_price,axis\u003d1))"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "weighted_fuel_price_feature \u003d grpd_by_regions_gallons_sum[[granularity_level,\u0027revenue_date\u0027,\u0027weighted_average_fuel_price\u0027,\u0027weighted_fuel_price_x_gallons\u0027]]\nweighted_fuel_price_feature \u003d weighted_fuel_price_feature.rename(columns\u003d{\u0027revenue_date\u0027:\u0027rev_date\u0027})"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "weighted_fuel_price_feature \u003d weighted_fuel_price_feature[weighted_fuel_price_feature[granularity_level].isin(grpd_by_regions_gallons_sum_required[granularity_level].unique())]"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pipeline"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# bill_codes_df \u003d pd.read_csv(\u0027D:/Users/W505723/WEX_Data_exploration/EDA_NewSanitisedData/Revenue Forecasting/revenue_code_groups.csv\u0027)\n# rebates_codes \u003d list(bill_codes_df[bill_codes_df[\u0027Revenue Code Group\u0027] \u003d\u003d \u0027Rebate\u0027][\u0027Revenue Code\u0027].values)\n# dataset_pd \u003d dataset_pd[dataset_pd[\u0027revenue_code\u0027].isin(rebates_codes)]\n\ndataset_pd \u003d dataset_pd[(dataset_pd[\u0027revenue_code\u0027]\u003d\u003d\u0027M01\u0027)]"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# def data_transform_to_id_level(ten_yrs_data,forecast_target_column,granularity_level):\n#     # grouping at id month level and calculating total_purchase_gallons_quantity/revenue\n#     grouped_id_level \u003d ten_yrs_data.groupby([granularity_level,\u0027revenue_date\u0027]).agg(total_values \u003d (forecast_target_column,sum))\n#     grouped_id_level.reset_index(inplace\u003dTrue)\n#     grouped_id_level \u003d grouped_id_level.groupby(granularity_level).agg(rev_date \u003d (\u0027revenue_date\u0027,list),gallons_list \u003d (\u0027total_values\u0027,list))\n\n#     grouped_id_level[\u0027total_records\u0027] \u003d grouped_id_level.apply(find_len,axis\u003d1)\n\n#     grouped_id_level.sort_values(\u0027total_records\u0027,ascending\u003dFalse,inplace \u003d True)\n\n#     return grouped_id_level"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def processing_engine(dataset_pd):\n\n    start_time \u003d time.time()\n    # Import and transform data to the desired level (WEX_ID-month level or business_program_name-month level)\n    grouped_id_level \u003d data_transform_to_id_level(dataset_pd,forecast_target_column,granularity_level)\n    # Account should have atleast 2 years of data to train check\n    if choice \u003d\u003d 1: #For only Forecast\n        print(1)\n        grouped_id_level[\u0027year_check\u0027]  \u003d grouped_id_level.apply(dates_checker_2,current_month\u003dpd.to_datetime(current_month),axis\u003d1)\n        grouped_id_level \u003d grouped_id_level[grouped_id_level[\u0027year_check\u0027]\u003d\u003d1]\n#         grouped_id_level\u003dgrouped_id_level.head(5)\n\n    if choice \u003d\u003d 2: #For Forecast and Compare\n        print(2)\n        grouped_id_level[\u0027year_check\u0027]  \u003d grouped_id_level.apply(dates_checker,current_month\u003dpd.to_datetime(current_month),axis\u003d1)\n        grouped_id_level \u003d grouped_id_level[grouped_id_level[\u0027year_check\u0027]\u003d\u003d1]\n#         grouped_id_level\u003dgrouped_id_level.head(30)\n\n    # Missing Value treatment\n    combined_date_list \u003d []\n    combined_gallons_list \u003d []\n    ans \u003d grouped_id_level.apply(padder,\n                                  combined_date_list\u003dcombined_date_list,\n                                  combined_gallons_list\u003dcombined_gallons_list,\n                                  validation_window_length\u003dvalidation_window_length,\n                                  current_month\u003dpd.to_datetime(current_month),axis\u003d1)\n    grouped_id_level[\u0027rev_date\u0027] \u003d combined_date_list\n    grouped_id_level[\u0027gallons_list\u0027] \u003d combined_gallons_list\n    grouped_id_level[\u0027total_records_after_padding\u0027] \u003d grouped_id_level.apply(find_len,axis\u003d1)\n    grouped_id_level_ind \u003d grouped_id_level.reset_index()\n\n    grouped_id_level_ind \u003d grouped_id_level_ind[grouped_id_level_ind[granularity_level].\n                                                              isin(grpd_by_regions_gallons_sum_required[granularity_level].\n                                                                   unique())]\n\n    print(grouped_id_level_ind.shape)\n    # Adding EFD as a feature\n    grouped_id_level_ind[\u0027REV_EQUIVALENT_FUEL_DAY_FACTOR_efd\u0027],\\\n    grouped_id_level_ind[\u0027REV_EQUIVALENT_FUEL_DAY_FACTOR_efd_ma3m\u0027],grouped_id_level_ind[\u0027REV_EQUIVALENT_FUEL_DAY_FACTOR_efd_ma6m\u0027]\u003d\\\n    zip(*grouped_id_level_ind.apply(merge_EFD_feature_file,\n                                             efd_features_monthly\u003defd_features_monthly,axis\u003d1))\n\n    # Adding covid mobility as a feature\n    grouped_id_level_ind[\u0027covid_mobility_feature\u0027]\u003dgrouped_id_level_ind.apply(merge_CMD_feature_file,\n                                                                                       cmd_features_monthly\u003dcovid_mobility_df,\n                                                                                       axis\u003d1)\n\n    # Adding fuel_price_data as a feature\n    grouped_id_level_ind[\u0027weighted_average_fuel_price\u0027],grouped_id_level_ind[\u0027weighted_fuel_price_x_gallons\u0027]\u003d\\\n    zip(*grouped_id_level_ind.apply(merge_weighted_fuel_price_feature_file,\n                                             weighted_fuel_price_feature\u003dweighted_fuel_price_feature,\n                                             granularity_level\u003dgranularity_level,\n                                             axis\u003d1))\n\n    print(\"--- %s seconds for data preparation---\" % (time.time() - start_time))\n    # Building models from the multiple options available\n    start_time \u003d time.time()\n    print(grouped_id_level_ind.shape)\n    models_trained \u003d []\n\n    if \u0027HW\u0027 in models:\n        start_time_hw \u003d time.time()\n        print(\u0027Running HW\u0027)\n        rows \u003d [(grouped_id_level_ind.loc[i],\n                 pd.to_datetime(current_month),\n                 validation_window_length,\n                 alpha,\n                 beta,\n                 gamma,\n                 granularity_level) for i in grouped_id_level_ind.index]\n        results_hw_list \u003d Parallel(n_jobs\u003dmp.cpu_count())(delayed(create_and_train_data_for_hw)(a,b,c,d,e,f,g) for a,b,c,d,e,f,g in rows)\n\n        results_hw \u003d pd.DataFrame(results_hw_list, columns\u003d[granularity_level, \u0027mape_hw\u0027, \u0027rmse_hw\u0027,\\\n                            \u0027agorithm_used_hw\u0027, \u0027rev_date_hw\u0027, \u0027ground_truth_hw\u0027, \u0027predictions_hw\u0027])\n        results_hw[\u0027agorithm_used_hw\u0027] \u003d \u0027HW\u0027\n        models_trained.append(results_hw)\n        del rows\n        gc.collect()\n        print(\"--- %s seconds for running HW---\" % (time.time() - start_time_hw))\n\n\n    if \u0027ARIMAX\u0027 in models and len(external_feature_list)\u003e0:\n        start_time_arimax \u003d time.time()\n        print(\u0027Running ARIMAX\u0027)\n        rows \u003d [(grouped_id_level_ind.loc[i],\n                 pd.to_datetime(current_month),\n                 validation_window_length,\n                 information_criterion,\n                 seasonal_arimax,\n                 external_feature_list,\n                 n_jobs,\n                 granularity_level) for i in grouped_id_level_ind.index]\n        results_arimax_list \u003d Parallel(n_jobs\u003dmp.cpu_count())(delayed(create_and_train_data_for_arimax)(a,b,c,d,e,f,g,h) for a,b,c,d,e,f,g,h in rows)\n        results_arimax \u003d pd.DataFrame(results_arimax_list, columns\u003d[\n            granularity_level, \u0027mape_arimax\u0027, \u0027rmse_arimax\u0027, \u0027agorithm_used_arimax\u0027,\n            \u0027rev_date_arimax\u0027, \u0027ground_truth_arimax\u0027, \u0027predictions_arimax\u0027\n        ])\n        results_arimax[\u0027agorithm_used_arimax\u0027] \u003d \u0027ARIMAX\u0027\n        models_trained.append(results_arimax)\n        del rows\n        gc.collect()\n        print(\"--- %s seconds for running ARIMAX---\" % (time.time() - start_time_arimax))\n    if \u0027XGB\u0027 in models and len(external_feature_list)\u003e0:\n        start_time_xgb \u003d time.time()\n        print(\u0027Running XGB\u0027)\n        rows \u003d [(grouped_id_level_ind.loc[i],\n                pd.to_datetime(current_month),\n                validation_window_length,\n                external_feature_list,\n                learning_rate_xgb,\n                n_estimators_xgb,\n                subsample_xgb,\n                max_depth_xgb,\n                colsample_bytree_xgb,\n                min_child_weight_xgb,\n                granularity_level) for i in grouped_id_level_ind.index]\n        results_xgb_list \u003d Parallel(n_jobs\u003dmp.cpu_count())(delayed(create_and_train_data_for_xgboost)(a,b,c,d,e,f,g,h,i,j,k) for a,b,c,d,e,f,g,h,i,j,k in rows)\n\n        results_xgb \u003d pd.DataFrame(results_xgb_list, columns\u003d[granularity_level, \u0027mape_xgb\u0027, \u0027rmse_xgb\u0027,\n                            \u0027agorithm_used_xgb\u0027, \u0027rev_date_xgb\u0027, \u0027ground_truth_xgb\u0027, \u0027predictions_xgb\u0027])\n        results_xgb[\u0027agorithm_used_xgb\u0027] \u003d \u0027XGB\u0027\n        models_trained.append(results_xgb)\n        del rows\n        gc.collect()\n        print(\"--- %s seconds for running XGB---\" % (time.time() - start_time_xgb))\n\n    if \u0027LSTM_Multivariate\u0027 in models and len(external_feature_list)\u003e0:\n        start_time_lstm \u003d time.time()\n        print(\u0027Running LSTM_Multivariate\u0027)\n        rows \u003d [(grouped_id_level_ind.loc[i],\n                 pd.to_datetime(current_month),\n                 validation_window_length,\n                 external_feature_list,\n                 train_period_lstm_model,\n                 pred_period_lstm_model,\n                 optimizer_lstm_model,\n                 loss_function_lstm_model,\n                 batch_size_lstm_model,\n                 num_epochs_lstm_model,\n                granularity_level) for i in grouped_id_level_ind.index]\n        results_lstm_list \u003d Parallel(n_jobs\u003dmp.cpu_count())(delayed(create_and_train_data_for_lstm_multivariate)(a,b,c,d,e,f,g,h,i,j,k) for a,b,c,d,e,f,g,h,i,j,k in rows)\n\n        results_lstm \u003d pd.DataFrame(results_lstm_list, columns\u003d[\n            granularity_level, \u0027mape_lstm_multivariate\u0027, \u0027rmse_lstm_multivariate\u0027,\n            \u0027agorithm_used_lstm_multivariate\u0027,\n            \u0027rev_date_lstm_multivariate\u0027, \u0027ground_truth_lstm_multivariate\u0027,\n            \u0027predictions_lstm_multivariate\u0027\n        ])\n        results_lstm[\u0027agorithm_used_lstm_multivariate\u0027] \u003d \u0027LSTM_Multivariate\u0027\n        models_trained.append(results_lstm)\n        del rows\n        gc.collect()\n        print(\"--- %s seconds for running LSTM_Multivariate---\" % (time.time() - start_time_lstm))\n\n    print(\"--- %s seconds for building models---\" % (time.time() - start_time))\n\n    ## COMPARE MODELS\n    print(\u0027Initialising Compare models\u0027)\n    start_time \u003d time.time()\n    id_best_algo_df \u003d compare_models(models_trained,granularity_level)\n    grouped_id_level_ind \u003d grouped_id_level_ind.merge(id_best_algo_df,on\u003dgranularity_level,how\u003d\u0027left\u0027)\n    print(\"--- %s seconds for comparing models---\" % (time.time() - start_time))\n\n    ## CHOOSE and FORECAST using the best model\n    print(\u0027Initialising Forecasting \u0027)\n    start_time \u003d time.time()\n\n    rows \u003d [(grouped_id_level_ind.loc[i],\n            pd.to_datetime(current_month),\n            validation_window_length,\n            information_criterion,\n            seasonal_arimax,\n            external_feature_list,\n            n_jobs,\n            learning_rate_xgb,\n            n_estimators_xgb,\n            subsample_xgb,\n            max_depth_xgb,\n            colsample_bytree_xgb,\n            min_child_weight_xgb,\n            train_period_lstm_model,\n            pred_period_lstm_model,\n            optimizer_lstm_model,\n            loss_function_lstm_model,\n            batch_size_lstm_model,\n            num_epochs_lstm_model,\n            alpha,\n            beta,\n            gamma,\n            granularity_level) for i in grouped_id_level_ind.index]\n    final_res_list \u003d Parallel(n_jobs\u003dmp.cpu_count())(delayed(choose_model_and_forecast)(a,b,c,d,e,f,g,h,i,j,k,l,m,n,o,p,q,r,s,t,u,v,w) for a,b,c,d,e,f,g,h,i,j,k,l,m,n,o,p,q,r,s,t,u,v,w in rows)\n\n    final_res \u003d pd.DataFrame(final_res_list, columns\u003d[granularity_level,\n                                                     \u0027overall_mape(performance_assessment_period)\u0027,\n                                                     \u0027overall_rmse(performance_assessment_period)\u0027,\n                                                     \u0027model_chosen\u0027,\n                                                     \u0027month\u0027,\n                                                     \u0027gallons_actual(performance_assessment_period)\u0027,\n                                                     \u0027gallons_forecasts(performance_assessment_period)\u0027])\n    del rows\n    gc.collect()\n    error_file \u003d final_res[final_res[\u0027model_chosen\u0027]\u003d\u003d\u0027ERROR\u0027]\n    final_res \u003d final_res[final_res[\u0027model_chosen\u0027]!\u003d\u0027ERROR\u0027]\n\n    #final_res[\u0027wex_id\u0027],final_res[\u0027performance_assesment_mape\u0027],final_res[\u0027performance_assesment_rmse\u0027],final_res[\u0027agorithm_used\u0027],final_res[\u0027performance_assesment_rev_date\u0027],final_res[\u0027performance_assesment_ground_truth\u0027],final_res[\u0027performance_assesment_forecasts\u0027],final_res[\u002724_months_forecasts\u0027]\u003d zip(*grouped_wex_id_level_ind.apply(choose_model_and_forecast,future_prediction_months\u003dfuture_prediction_months,current_month\u003dcurrent_month,performance_assessment_window\u003dperformance_assessment_window,trend\u003dtrend,seasonal_hw\u003dseasonal_hw,seasonal_periods\u003dseasonal_period,information_criterion\u003dinformation_criterion,seasonal_arimax\u003dseasonal_arimax,external_feature_list\u003dexternal_feature_list,n_jobs\u003dn_jobs,learning_rate_xgb\u003dlearning_rate_xgb,n_estimators_xgb\u003dn_estimators_xgb,subsample_xgb\u003dsubsample_xgb,max_depth_xgb\u003dmax_depth_xgb,colsample_bytree_xgb\u003dcolsample_bytree_xgb,min_child_weight_xgb\u003dmin_child_weight_xgb,axis\u003d1))\n    print(\"--- %s seconds for forecasting using best model---\" % (time.time() - start_time))\n\n    ## Output the required files\n    start_time \u003d time.time()\n    final_res[\u00273_month_mape(performance_assesment_period)\u0027] \u003d final_res.apply(three_months_mape_calc,axis\u003d1)\n    final_res[\u0027time_weighted_mape(performance_assesment_period)\u0027] \u003d final_res.apply(weighted_mape_calc,axis\u003d1)\n\n    final_res[\u0027overall_mape(performance_assessment_period)\u0027] \u003d final_res[\u0027overall_mape(performance_assessment_period)\u0027].astype(\u0027float64\u0027)\n    final_res[\u0027time_weighted_mape(performance_assesment_period)\u0027] \u003d final_res[\u0027time_weighted_mape(performance_assesment_period)\u0027].astype(\u0027float64\u0027)\n    final_res[\u00273_month_mape(performance_assesment_period)\u0027] \u003d final_res[\u00273_month_mape(performance_assesment_period)\u0027].astype(\u0027float64\u0027)\n\n    final_res[\u0027overall_mape(performance_assessment_period)\u0027] \u003d round(final_res[\u0027overall_mape(performance_assessment_period)\u0027],2)\n    final_res[\u0027time_weighted_mape(performance_assesment_period)\u0027] \u003d round(final_res[\u0027time_weighted_mape(performance_assesment_period)\u0027],2)\n    final_res[\u00273_month_mape(performance_assesment_period)\u0027] \u003d round(final_res[\u00273_month_mape(performance_assesment_period)\u0027],2)\n    df_dict \u003d dict()\n    for idx,dfs in enumerate(models_trained):\n        model \u003d dfs.iloc[0,3]\n        df_dict[model] \u003d idx\n\n    final_res[\u0027overall_mape(evaluation_period)\u0027],final_res[\u00273_month_mape(evaluation_period)\u0027],\\\n    final_res[\u0027time_weighted_mape(evaluation_period)\u0027],final_res[\u0027monthly_avg_volume(evaluation_period)\u0027] \u003d\\\n    zip(*final_res.apply(records_maintainer,\n                                  df_dict\u003ddf_dict,models_trained\u003dmodels_trained,\n                                  granularity_level\u003dgranularity_level,axis\u003d1))\n\n    metrics_file \u003d final_res[[granularity_level,\u0027model_chosen\u0027,\u0027monthly_avg_volume(evaluation_period)\u0027,\u0027overall_mape(evaluation_period)\u0027,\n                             \u00273_month_mape(evaluation_period)\u0027,\u0027time_weighted_mape(evaluation_period)\u0027,\n                             \u0027overall_mape(performance_assessment_period)\u0027,\u00273_month_mape(performance_assesment_period)\u0027,\n                             \u0027time_weighted_mape(performance_assesment_period)\u0027]]\n\n\n    final_res \u003d final_res.merge(grouped_id_level_ind[[granularity_level,\u0027rev_date\u0027,\u0027gallons_list\u0027]], on \u003dgranularity_level,how\u003d\u0027left\u0027)\n    metric_file \u003d pd.DataFrame()\n    metric_file[\u0027data\u0027] \u003d final_res.apply(metric_file_generator,\n                                                   df_dict\u003ddf_dict, models_trained\u003dmodels_trained,models\u003dmodels,\n                                                   granularity_level\u003dgranularity_level,axis\u003d1)\n    total_forecast_file \u003d pd.DataFrame()\n    for i in range(len(metric_file)):\n        total_forecast_file \u003d total_forecast_file.append(metric_file.iloc[i][0],ignore_index\u003dTrue)\n\n    ##Exception accounts\n    print(\u0027Handling Exception accounts\u0027)\n    final_res_B \u003d final_res[final_res[\u0027overall_mape(evaluation_period)\u0027]\u003e\u003d40]\n    final_res11gt40_wids \u003d list(final_res_B[granularity_level])\n\n    grouped_id_level \u003d data_transform_to_id_level(dataset_pd,forecast_target_column,granularity_level)\n    if choice \u003d\u003d 1:\n        print(1)\n        grouped_id_level[\u0027year_check\u0027]  \u003d grouped_id_level.apply(dates_checker_2,current_month\u003dpd.to_datetime(current_month),axis\u003d1)\n    if choice \u003d\u003d 2:\n        print(2)\n        grouped_id_level[\u0027year_check\u0027]  \u003d grouped_id_level.apply(dates_checker,current_month\u003dpd.to_datetime(current_month),axis\u003d1)\n\n    grouped_id_level_ind_A \u003d grouped_id_level.reset_index()\n\n    grouped_id_level_ind_A[\u0027ev_mape_gt40_check\u0027]  \u003d np.where((grouped_id_level_ind_A[granularity_level].isin(final_res11gt40_wids)),1,0)\n    grouped_id_level \u003d grouped_id_level_ind_A.set_index(granularity_level)\n\n    grouped_id_level_AB \u003d grouped_id_level[(grouped_id_level[\u0027year_check\u0027]\u003d\u003d2) | (grouped_id_level[\u0027ev_mape_gt40_check\u0027]\u003d\u003d1)]\n\n    #Padding\n    combined_date_list_AB \u003d []\n    combined_gallons_list_AB \u003d []\n    ans_AB \u003d grouped_id_level_AB.apply(padder,\n                                                combined_date_list\u003dcombined_date_list_AB,\n                                                combined_gallons_list\u003dcombined_gallons_list_AB,\n                                                validation_window_length\u003dvalidation_window_length,\n                                                current_month\u003dpd.to_datetime(current_month),axis\u003d1)\n    grouped_id_level_AB[\u0027rev_date\u0027] \u003d combined_date_list_AB\n    grouped_id_level_AB[\u0027gallons_list\u0027] \u003d combined_gallons_list_AB\n    grouped_id_level_AB[\u0027total_records_after_padding\u0027] \u003d grouped_id_level_AB.apply(find_len,axis\u003d1)\n    grouped_id_level_ind_AB \u003d grouped_id_level_AB.reset_index()\n#     grouped_id_level_ind_AB[\u0027state\u0027] \u003d grouped_id_level_ind_AB.apply(state_length_check,axis\u003d1)\n    grouped_id_level_ind_AB \u003d grouped_id_level_ind_AB[grouped_id_level_ind_AB[\u0027total_records_after_padding\u0027]\u003e\u003d18]\n\n    ##storing ids\n    smallaccounts_ids \u003d list(grouped_id_level_ind_AB[grouped_id_level_ind_AB[\u0027year_check\u0027]\u003d\u003d2][granularity_level])\n\n    grouped_id_level_ind_AB[\u0027REV_EQUIVALENT_FUEL_DAY_FACTOR_efd\u0027],\\\n    grouped_id_level_ind_AB[\u0027REV_EQUIVALENT_FUEL_DAY_FACTOR_efd_ma3m\u0027],\\\n    grouped_id_level_ind_AB[\u0027REV_EQUIVALENT_FUEL_DAY_FACTOR_efd_ma6m\u0027]\u003d\\\n    zip(*grouped_id_level_ind_AB.apply(merge_EFD_feature_file,efd_features_monthly\u003defd_features_monthly,axis\u003d1))\n\n    # Adding covid mobility as a feature\n    grouped_id_level_ind_AB[\u0027covid_mobility_feature\u0027]\u003dgrouped_id_level_ind_AB.apply(merge_CMD_feature_file,\n                                                                                       cmd_features_monthly\u003dcovid_mobility_df,\n                                                                                       axis\u003d1)\n\n    # Adding fuel_price_data as a feature\n    grouped_id_level_ind_AB[\u0027weighted_average_fuel_price\u0027],grouped_id_level_ind_AB[\u0027weighted_fuel_price_x_gallons\u0027]\u003d\\\n    zip(*grouped_id_level_ind_AB.apply(merge_weighted_fuel_price_feature_file,\n                                             weighted_fuel_price_feature\u003dweighted_fuel_price_feature,\n                                             granularity_level\u003dgranularity_level,\n                                             axis\u003d1))\n\n    ## Modelling for Exception accounts\n#     grouped_id_level_ind_AB \u003d grouped_id_level_ind_AB.sample(5)\n    #ARIMAX\n    grouped_id_level_ind_AB \u003d grouped_id_level_ind_AB.assign(best_algo\u003d\u0027ARIMAX\u0027)   #model_for_exception_accounts\n    rows \u003d [(grouped_id_level_ind_AB.loc[i],\n            pd.to_datetime(current_month),\n            validation_window_length,\n            information_criterion,\n            seasonal_arimax,\n            external_feature_list,\n            n_jobs,\n            learning_rate_xgb,\n            n_estimators_xgb,\n            subsample_xgb,\n            max_depth_xgb,\n            colsample_bytree_xgb,\n            min_child_weight_xgb,\n            train_period_lstm_model,\n            pred_period_lstm_model,\n            optimizer_lstm_model,\n            loss_function_lstm_model,\n            batch_size_lstm_model,\n            num_epochs_lstm_model,\n            alpha,\n            beta,\n            gamma,\n            granularity_level) for i in grouped_id_level_ind_AB.index]\n    final_res_exp_list_1 \u003d Parallel(n_jobs\u003dmp.cpu_count())(delayed(choose_model_and_forecast)(a,b,c,d,e,f,g,h,i,j,k,l,m,n,o,p,q,r,s,t,u,v,w) for a,b,c,d,e,f,g,h,i,j,k,l,m,n,o,p,q,r,s,t,u,v,w in rows)\n\n    final_res_exp_1 \u003d pd.DataFrame(final_res_exp_list_1, columns\u003d[granularity_level,\n                                                     \u0027overall_mape(performance_assessment_period)\u0027,\n                                                     \u0027overall_rmse(performance_assessment_period)\u0027,\n                                                     \u0027model_chosen\u0027,\n                                                     \u0027month\u0027,\n                                                     \u0027gallons_actual(performance_assessment_period)\u0027,\n                                                     \u0027gallons_forecasts(performance_assessment_period)\u0027])\n    del rows\n    gc.collect()\n\n    #LSTM_MULTIVARIATE\n    grouped_id_level_ind_AB \u003d grouped_id_level_ind_AB.assign(best_algo\u003d\u0027LSTM_MULTIVARIATE\u0027)   #model_for_exception_accounts\n    train_period_lstm_model_exp \u003d 5\n    rows \u003d [(grouped_id_level_ind_AB.loc[i],\n            pd.to_datetime(current_month),\n            validation_window_length,\n            information_criterion,\n            seasonal_arimax,\n            external_feature_list,\n            n_jobs,\n            learning_rate_xgb,\n            n_estimators_xgb,\n            subsample_xgb,\n            max_depth_xgb,\n            colsample_bytree_xgb,\n            min_child_weight_xgb,\n            train_period_lstm_model_exp,\n            pred_period_lstm_model,\n            optimizer_lstm_model,\n            loss_function_lstm_model,\n            batch_size_lstm_model,\n            num_epochs_lstm_model,\n            alpha,\n            beta,\n            gamma,\n            granularity_level) for i in grouped_id_level_ind_AB.index]\n    final_res_exp_list_2 \u003d Parallel(n_jobs\u003dmp.cpu_count())(delayed(choose_model_and_forecast)(a,b,c,d,e,f,g,h,i,j,k,l,m,n,o,p,q,r,s,t,u,v,w) for a,b,c,d,e,f,g,h,i,j,k,l,m,n,o,p,q,r,s,t,u,v,w in rows)\n\n    final_res_exp_2 \u003d pd.DataFrame(final_res_exp_list_2, columns\u003d[granularity_level,\n                                                     \u0027overall_mape(performance_assessment_period)\u0027,\n                                                     \u0027overall_rmse(performance_assessment_period)\u0027,\n                                                     \u0027model_chosen\u0027,\n                                                     \u0027month\u0027,\n                                                     \u0027gallons_actual(performance_assessment_period)\u0027,\n                                                     \u0027gallons_forecasts(performance_assessment_period)\u0027])\n    del rows\n    gc.collect()\n\n    #ENSEMBLER\n    final_res_exp \u003d pd.DataFrame()\n    gallons_forecasts \u003d []\n\n    for i in range(final_res_exp_2.shape[0]):\n        gallons_forecasts.append(mean_calc(final_res_exp_1[\u0027gallons_forecasts(performance_assessment_period)\u0027].iloc[i],final_res_exp_2[\u0027gallons_forecasts(performance_assessment_period)\u0027].iloc[i]))\n    final_res_exp[granularity_level] \u003d final_res_exp_2[granularity_level]\n    final_res_exp[\u0027gallons_actual(performance_assessment_period)\u0027] \u003d final_res_exp_2[\u0027gallons_actual(performance_assessment_period)\u0027]\n    final_res_exp[\u0027gallons_forecasts(performance_assessment_period)\u0027] \u003d gallons_forecasts\n    final_res_exp[\u0027model_chosen\u0027] \u003d \u0027ENSEMBLE(LSTM_and_ARIMAX)\u0027\n    final_res_exp[\u0027month\u0027] \u003d final_res_exp_2[\u0027month\u0027]\n    final_res_exp[\u0027overall_mape(performance_assessment_period)\u0027] \u003d final_res_exp.apply(twelve_months_mape_calc,axis\u003d1)\n    final_res_exp[\u0027overall_rmse(performance_assessment_period)\u0027] \u003d final_res_exp.apply(twelve_months_rmse_calc,axis\u003d1)\n\n    error_file_exp \u003d final_res_exp[final_res_exp[\u0027model_chosen\u0027]\u003d\u003d\u0027ERROR\u0027]\n    final_res_exp \u003d final_res_exp[final_res_exp[\u0027model_chosen\u0027]!\u003d\u0027ERROR\u0027]\n\n    final_res_exp[\u00273_month_mape(performance_assesment_period)\u0027] \u003d final_res_exp.apply(three_months_mape_calc,axis\u003d1)\n    final_res_exp[\u0027time_weighted_mape(performance_assesment_period)\u0027] \u003d final_res_exp.apply(weighted_mape_calc,axis\u003d1)\n\n    final_res_exp \u003d final_res_exp.merge(grouped_id_level_ind_AB[[granularity_level,\u0027rev_date\u0027,\u0027gallons_list\u0027]],on\u003dgranularity_level,how\u003d\u0027left\u0027)\n    metrics_fileAB \u003d final_res_exp[[granularity_level,\u0027model_chosen\u0027,\u0027overall_mape(performance_assessment_period)\u0027,\n                              \u00273_month_mape(performance_assesment_period)\u0027,\u0027time_weighted_mape(performance_assesment_period)\u0027]]\n\n    metric_fileAB \u003d pd.DataFrame()\n    metric_fileAB[\u0027data\u0027] \u003d final_res_exp.apply(metric_file_generator_for_exception_accounts,granularity_level\u003dgranularity_level,axis\u003d1)\n    total_forecast_fileAB \u003d pd.DataFrame()\n    for i in range(len(metric_fileAB)):\n        total_forecast_fileAB \u003d total_forecast_fileAB.append(metric_fileAB.iloc[i][0],ignore_index\u003dTrue)\n\n    exception_ids \u003d list(final_res_exp[granularity_level])\n    metrics_file \u003d metrics_file[~(metrics_file[granularity_level].isin(exception_ids))]\n    total_forecast_file \u003d total_forecast_file[~(total_forecast_file[granularity_level].isin(exception_ids))]\n    metrics_file \u003d metrics_file[~(metrics_file[granularity_level].isin(exception_ids))]\n    final_res \u003d final_res[~(final_res[granularity_level].isin(exception_ids))]\n\n    total_forecast_file \u003d total_forecast_file.append(total_forecast_fileAB).reset_index(drop\u003dTrue)\n    metrics_file \u003d metrics_file.append(metrics_fileAB).reset_index(drop\u003dTrue)\n    final_res \u003d final_res.append(final_res_exp).reset_index(drop\u003dTrue)\n\n    ## Putting status  flags to differentiate different accounts\n    conditions \u003d [final_res[granularity_level].isin(final_res11gt40_wids),\n                  final_res[granularity_level].isin(smallaccounts_ids)]\n    choices \u003d    [\u0027accounts with greater than 40 evaluation period MAPE\u0027,\n                  \u0027accounts with less than 36 datapoints\u0027]\n    final_res[\u0027status_flag\u0027] \u003d np.select(conditions, choices, default\u003d\u0027eligible accounts\u0027)\n\n    conditions \u003d [total_forecast_file[granularity_level].isin(final_res11gt40_wids),\n                  total_forecast_file[granularity_level].isin(smallaccounts_ids)]\n    choices \u003d    [\u0027accounts with greater than 40 evaluation period MAPE\u0027,\n                  \u0027accounts with less than 36 datapoints\u0027]\n    total_forecast_file[\u0027status_flag\u0027] \u003d np.select(conditions, choices, default\u003d\u0027eligible accounts\u0027)\n\n    conditions \u003d [metrics_file[granularity_level].isin(final_res11gt40_wids),\n                  metrics_file[granularity_level].isin(smallaccounts_ids)]\n    choices \u003d    [\u0027accounts with greater than 40 evaluation period MAPE\u0027,\n                  \u0027accounts with less than 36 datapoints\u0027]\n    metrics_file[\u0027status_flag\u0027] \u003d np.select(conditions, choices, default\u003d\u0027eligible accounts\u0027)\n\n    if choice \u003d\u003d 1:\n        print(1)\n        metrics_file.drop([\u0027overall_mape(performance_assessment_period)\u0027,\n                           \u00273_month_mape(performance_assesment_period)\u0027,\n                           \u0027time_weighted_mape(performance_assesment_period)\u0027],axis\u003d1,inplace\u003dTrue)\n\n#     metrics_file.to_csv(\u0027metrics_file.csv\u0027,index\u003dFalse)\n#     total_forecast_file.to_csv(\u0027forecasts.csv\u0027,index\u003dFalse)\n#     final_res.to_csv(\u0027final_results.csv\u0027,index\u003dFalse)\n\n    # Write recipe outputs\n    metrics \u003d dataiku.Dataset(\"metrics\")\n    metrics.write_with_schema(metrics_file)\n    forecasts \u003d dataiku.Dataset(\"forecasts\")\n    forecasts.write_with_schema(total_forecast_file)\n\n    print(\"--- %s seconds for outputting the required files---\" % (time.time() - start_time))\n\n\n    return models_trained,grouped_id_level_ind,id_best_algo_df,final_res,total_forecast_file,metrics_file,error_file,error_file_exp"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "models_trained,grouped_id_level_ind,id_best_algo_df,final_res,\\\ntotal_forecast_file,metrics_file,error_file,error_file_exp\u003d processing_engine(dataset_pd)"
      ],
      "outputs": []
    }
  ]
}