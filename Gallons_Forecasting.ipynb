{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a2957fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd8dd97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import itertools\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date\n",
    "import os\n",
    "import yaml\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import argparse\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "%matplotlib inline\n",
    "import plotly.express as px\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from pmdarima.arima import auto_arima\n",
    "from functools import reduce\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from statsmodels.tsa.holtwinters import SimpleExpSmoothing   \n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "### Create the Stacked LSTM model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,LSTM,GRU,Dropout\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pmdarima.arima import auto_arima\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import holidays\n",
    "from collections import defaultdict\n",
    "import statsmodels.api as sm\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "from helper_mp import find_len,state_length_check,data_transform_to_id_level,\\\n",
    "dates_checker,dates_checker_2,padder,mean_absolute_percentage_error,create_dataset,create_lstm,\\\n",
    "fit_model,create_and_train_data_for_lstm,create_and_train_data_for_hw,create_and_train_data_for_arimax,\\\n",
    "create_and_train_data_for_xgboost,compare_models,choose_model_and_forecast,read_params,get_data,\\\n",
    "three_months_mape_calc,weighted_mape_calc,efd_feature_generator,merge_EFD_feature_file,\\\n",
    "create_and_train_data_for_lstm_multivariate,records_maintainer,metric_file_generator,\\\n",
    "create_and_train_data_for_hwwt,create_and_train_data_for_hwws,merge_cardcounts_feature_file,\\\n",
    "metric_file_generator_for_exception_accounts,merge_CMD_feature_file,twelve_months_mape_calc,twelve_months_rmse_calc,mean_calc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d164c006",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import gc\n",
    "from joblib import Parallel, delayed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b42e14f",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9fed043",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdm_file_path,raw_data_ten_years,feature_file_path,revenue_date_EFD_by_day_path,daily_gallons_full_path,window_size_lstm,optimizer,loss,models,granularity_level,seasonal_period,trend,seasonal_hw,information_criterion,seasonal_arimax,n_jobs,current_month,validation_window_length,future_prediction_months,date_year,date_month,forecast_target_column,performance_assessment_window,lstm_train_window_length_EFD,num_units_EFD,activation_function_EFD,optimizer_EFD,loss_function_EFD,batch_size_EFD,num_epochs_EFD,MODELS_EFD,ma_mapping_EFD,external_feature_list,learning_rate_xgb,n_estimators_xgb,subsample_xgb,max_depth_xgb,colsample_bytree_xgb,min_child_weight_xgb,train_period_lstm_model,pred_period_lstm_model,num_units_lstm_model,optimizer_lstm_model,loss_function_lstm_model,batch_size_lstm_model,num_epochs_lstm_model,alpha,beta,gamma,card_counts_full_path,model_for_exception_accounts,choice,business_program_name= get_data(config_path=\"params17.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81229901",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('D:/Users/W505723/Downloads/Sanitized_dataset/NewSanitisedData/mdm_final_table_sanitized.csv',\n",
       " 'D:/Users/W505723/Downloads/Sanitized_dataset/EDW_{}_volume_Sanitized.csv',\n",
       " 'D:/Users/W505723/WEX_Data_exploration/EDA_NewSanitisedData/grouped_efd_per_month.csv',\n",
       " 'D:/Users/W505723/WEX_Data_exploration/EDA_NewSanitisedData/mv_features/revenue_date_EFD_by_day.txt',\n",
       " 'D:/Users/W505723/WEX_Data_exploration/EDA_NewSanitisedData/mv_features/daily_gallons_full.csv',\n",
       " 'D:/Users/W505723/WEX_Data_exploration/EDA_NewSanitisedData/mv_features/card_counts_sanitized.csv',\n",
       " 36,\n",
       " 'adam',\n",
       " 'mean_squared_error',\n",
       " ['HW', 'ARIMAX', 'XGB', 'LSTM_Multivariate'],\n",
       " 'partyparentid',\n",
       " 12,\n",
       " 'mul',\n",
       " 'mul',\n",
       " 'aic',\n",
       " True,\n",
       " -1,\n",
       " datetime.date(2018, 12, 1),\n",
       " 12,\n",
       " 24,\n",
       " 'revenue_year',\n",
       " 'revenue_month',\n",
       " 'puchase_gallons_qty',\n",
       " 12,\n",
       " 24,\n",
       " 16,\n",
       " 'relu',\n",
       " 'adam',\n",
       " 'mean_squared_error',\n",
       " 16,\n",
       " 100,\n",
       " ['efd', 'efd_ma3m', 'efd_ma6m'],\n",
       " {'efd_ma3m': 91, 'efd_ma6m': 182},\n",
       " ['lag_12',\n",
       "  'lag_9',\n",
       "  'lag_6',\n",
       "  'lag_3',\n",
       "  'lag_2',\n",
       "  'lag_1',\n",
       "  'MA12',\n",
       "  'MA6',\n",
       "  'MSD12',\n",
       "  'MSD6',\n",
       "  'REV_EQUIVALENT_FUEL_DAY_FACTOR_efd_ma3m',\n",
       "  'covid_mobility_feature'],\n",
       " [0.01, 0.02, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5],\n",
       " [10, 30, 50, 100, 300, 500, 600, 700, 900, 1000],\n",
       " [0.1, 0.2, 0.3, 0.5, 0.7, 0.9, 1],\n",
       " [1, 3, 5],\n",
       " [0.5, 0.7, 0.9, 1],\n",
       " [1, 2, 3],\n",
       " 9,\n",
       " 1,\n",
       " 64,\n",
       " 'adam',\n",
       " 'mean_squared_error',\n",
       " 10,\n",
       " 70,\n",
       " 0.3,\n",
       " 0.16,\n",
       " 0.3,\n",
       " 'ARIMAX',\n",
       " 2,\n",
       " 'Wright Express Fleet Universal')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdm_file_path,raw_data_ten_years,feature_file_path,revenue_date_EFD_by_day_path,daily_gallons_full_path,card_counts_full_path,window_size_lstm,optimizer,loss,models,granularity_level,seasonal_period,trend,seasonal_hw,information_criterion,seasonal_arimax,n_jobs,current_month,validation_window_length,future_prediction_months,date_year,date_month,forecast_target_column,performance_assessment_window,lstm_train_window_length_EFD,num_units_EFD,activation_function_EFD,optimizer_EFD,loss_function_EFD,batch_size_EFD,num_epochs_EFD,MODELS_EFD,ma_mapping_EFD,external_feature_list,learning_rate_xgb,n_estimators_xgb,subsample_xgb,max_depth_xgb,colsample_bytree_xgb,min_child_weight_xgb,train_period_lstm_model,pred_period_lstm_model,num_units_lstm_model,optimizer_lstm_model,loss_function_lstm_model,batch_size_lstm_model,num_epochs_lstm_model,alpha,beta,gamma,model_for_exception_accounts,choice,business_program_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5150eef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5717ee51155a4574ad95eda996822575",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#loading ten years raw data\n",
    "dataset_pd = pd.DataFrame()\n",
    "for i in tqdm(range(2010,2023,1)):\n",
    "    df = pd.read_csv(raw_data_ten_years.format(i))\n",
    "    df = df.dropna(subset=['customer_account_id'])\n",
    "    dataset_pd = pd.concat([dataset_pd, df], ignore_index=True)\n",
    "\n",
    "## data pre processing\n",
    "dataset_pd['customer_account_id'] = dataset_pd['customer_account_id'].astype('int64')\n",
    "dataset_pd['revenue_date'] = pd.to_datetime(dataset_pd['revenue_date'])\n",
    "dataset_pd.sort_values(['revenue_date'], inplace=True)\n",
    "\n",
    "#loading mdm data\n",
    "mdm = pd.read_csv(mdm_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e9e9ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_pd = dataset_pd[dataset_pd['customer_business_program_name']==business_program_name]\n",
    "\n",
    "\n",
    "mdm = mdm[mdm['businessprogramname']==business_program_name]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a23b16e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3166338, 13), (82374, 29))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_pd[forecast_target_column] = np.where(dataset_pd[forecast_target_column] < 0, 0, dataset_pd[forecast_target_column])\n",
    "\n",
    "dataset_pd.shape,mdm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98906902",
   "metadata": {},
   "source": [
    "## EFD Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a21dd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Main holidays\n",
    "holidays_me = pd.DataFrame()\n",
    "for yr in range(2008, 2025, 1):\n",
    "    me_cal = pd.DataFrame(holidays.US(state='ME', years=yr).items(), columns = ['REV_CALENDAR_DATE', 'HOLIDAY_NAME'])\n",
    "    me_cal['REV_CALENDAR_DATE'] = pd.to_datetime(me_cal['REV_CALENDAR_DATE'])\n",
    "    me_cal['HOLIDAY'] = 1\n",
    "    holidays_me = pd.concat([holidays_me, me_cal])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6478c7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-01-01 00:00:00 2017-12-31 00:00:00 2018-01-01 00:00:00 2018-12-31 00:00:00 2019-01-01 00:00:00 2019-12-31 00:00:00\n",
      "371\n",
      "371\n",
      "371\n"
     ]
    }
   ],
   "source": [
    "efd_features_monthly = efd_feature_generator(holidays_me,str(current_month),lstm_train_window_length_EFD,num_units_EFD,activation_function_EFD,optimizer_EFD,loss_function_EFD,batch_size_EFD,num_epochs_EFD,MODELS_EFD,ma_mapping_EFD,revenue_date_EFD_by_day_path,daily_gallons_full_path,validation_window_length,future_prediction_months)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1c5fba05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rev_date</th>\n",
       "      <th>REV_EQUIVALENT_FUEL_DAY_FACTOR_efd</th>\n",
       "      <th>REV_EQUIVALENT_FUEL_DAY_FACTOR_efd_ma3m</th>\n",
       "      <th>REV_EQUIVALENT_FUEL_DAY_FACTOR_efd_ma6m</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>2019-08-01</td>\n",
       "      <td>24.317775</td>\n",
       "      <td>23.541731</td>\n",
       "      <td>23.154284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>2019-09-01</td>\n",
       "      <td>22.735426</td>\n",
       "      <td>22.539486</td>\n",
       "      <td>22.188164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>2019-10-01</td>\n",
       "      <td>25.260967</td>\n",
       "      <td>23.850849</td>\n",
       "      <td>23.470979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>2019-11-01</td>\n",
       "      <td>22.434596</td>\n",
       "      <td>21.292680</td>\n",
       "      <td>20.994602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>2019-12-01</td>\n",
       "      <td>22.589182</td>\n",
       "      <td>22.916341</td>\n",
       "      <td>22.306160</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      rev_date  REV_EQUIVALENT_FUEL_DAY_FACTOR_efd  \\\n",
       "109 2019-08-01                           24.317775   \n",
       "110 2019-09-01                           22.735426   \n",
       "111 2019-10-01                           25.260967   \n",
       "112 2019-11-01                           22.434596   \n",
       "113 2019-12-01                           22.589182   \n",
       "\n",
       "     REV_EQUIVALENT_FUEL_DAY_FACTOR_efd_ma3m  \\\n",
       "109                                23.541731   \n",
       "110                                22.539486   \n",
       "111                                23.850849   \n",
       "112                                21.292680   \n",
       "113                                22.916341   \n",
       "\n",
       "     REV_EQUIVALENT_FUEL_DAY_FACTOR_efd_ma6m  \n",
       "109                                23.154284  \n",
       "110                                22.188164  \n",
       "111                                23.470979  \n",
       "112                                20.994602  \n",
       "113                                22.306160  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "efd_features_monthly.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1b5ba468",
   "metadata": {},
   "outputs": [],
   "source": [
    "efd_features_monthly=efd_features_monthly.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ee069f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "efd_features_monthly = efd_features_monthly.rename(columns = {'index':'rev_date'})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dd189b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "efd_features_monthly['rev_date'] = pd.to_datetime(efd_features_monthly['rev_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "76622c5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rev_date</th>\n",
       "      <th>REV_EQUIVALENT_FUEL_DAY_FACTOR_efd</th>\n",
       "      <th>REV_EQUIVALENT_FUEL_DAY_FACTOR_efd_ma3m</th>\n",
       "      <th>REV_EQUIVALENT_FUEL_DAY_FACTOR_efd_ma6m</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>2019-08-01</td>\n",
       "      <td>24.317775</td>\n",
       "      <td>23.541731</td>\n",
       "      <td>23.154284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>2019-09-01</td>\n",
       "      <td>22.735426</td>\n",
       "      <td>22.539486</td>\n",
       "      <td>22.188164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>2019-10-01</td>\n",
       "      <td>25.260967</td>\n",
       "      <td>23.850849</td>\n",
       "      <td>23.470979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>2019-11-01</td>\n",
       "      <td>22.434596</td>\n",
       "      <td>21.292680</td>\n",
       "      <td>20.994602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>2019-12-01</td>\n",
       "      <td>22.589182</td>\n",
       "      <td>22.916341</td>\n",
       "      <td>22.306160</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      rev_date  REV_EQUIVALENT_FUEL_DAY_FACTOR_efd  \\\n",
       "109 2019-08-01                           24.317775   \n",
       "110 2019-09-01                           22.735426   \n",
       "111 2019-10-01                           25.260967   \n",
       "112 2019-11-01                           22.434596   \n",
       "113 2019-12-01                           22.589182   \n",
       "\n",
       "     REV_EQUIVALENT_FUEL_DAY_FACTOR_efd_ma3m  \\\n",
       "109                                23.541731   \n",
       "110                                22.539486   \n",
       "111                                23.850849   \n",
       "112                                21.292680   \n",
       "113                                22.916341   \n",
       "\n",
       "     REV_EQUIVALENT_FUEL_DAY_FACTOR_efd_ma6m  \n",
       "109                                23.154284  \n",
       "110                                22.188164  \n",
       "111                                23.470979  \n",
       "112                                20.994602  \n",
       "113                                22.306160  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "efd_features_monthly.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a56472",
   "metadata": {},
   "source": [
    "## Google mobility feature for covid months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "32e11eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_mobility_feature(current_month, future_prediction_months,\n",
    "                              start_date = '2010-07-01', \n",
    "                              state_name = None,\n",
    "                              mobility_col = 'retail_and_recreation_percent_change_from_baseline',\n",
    "                              pre_covid_baseline = 0,\n",
    "                              post_covid_baseline = -3\n",
    "                             ):\n",
    "    '''\n",
    "        Function to generate mobility feature using Google mobility data for the US\n",
    "        state_name: Add complete name of state, eg. 'California', to get state level mobility feature, \n",
    "                    else None for national level mobility feature\n",
    "        mobility_col: Select the mobility type (needs to be the exact column name from the file)\n",
    "        pre-covid baseline: This value will be reflected for all pre-covid months\n",
    "        post-covid baseline: This value will be reflected for all forecasting window months\n",
    "    '''\n",
    "    import zipfile\n",
    "    from urllib.request import urlopen\n",
    "    import shutil\n",
    "    import os\n",
    "    from dateutil.relativedelta import relativedelta\n",
    "\n",
    "    url = \"https://www.gstatic.com/covid19/mobility/Region_Mobility_Report_CSVs.zip\"\n",
    "    file_name = 'Region_Mobility_Report_CSVs.zip'\n",
    "\n",
    "    # extracting zipfile from URL\n",
    "    with urlopen(url) as response, open(file_name, 'wb') as out_file:\n",
    "        shutil.copyfileobj(response, out_file)\n",
    "\n",
    "        # extracting required file from zipfile\n",
    "        with zipfile.ZipFile(file_name) as zf:\n",
    "            zf.extract('2020_US_Region_Mobility_Report.csv')\n",
    "            zf.extract('2021_US_Region_Mobility_Report.csv')\n",
    "            zf.extract('2022_US_Region_Mobility_Report.csv')\n",
    "\n",
    "    # deleting the zipfile from the directory\n",
    "    os.remove('Region_Mobility_Report_CSVs.zip')\n",
    "\n",
    "    # loading data from the files\n",
    "    mobility = pd.read_csv('2020_US_Region_Mobility_Report.csv').append(\n",
    "                pd.read_csv('2021_US_Region_Mobility_Report.csv')).append(\n",
    "                pd.read_csv('2022_US_Region_Mobility_Report.csv')).rename(\n",
    "                columns = {'sub_region_1':'state'})\n",
    "\n",
    "    if state_name == None:\n",
    "        mobility = mobility[(mobility.country_region_code == 'US') & (mobility.state.isnull()) & (mobility.sub_region_2.isnull())][[\n",
    "                'date', mobility_col\n",
    "            ]]\n",
    "    else:\n",
    "        mobility = mobility[(mobility.country_region_code == 'US') & (mobility.state==state_name) & (mobility.sub_region_2.isnull())][[\n",
    "                'date', mobility_col\n",
    "            ]]\n",
    "\n",
    "    mobility = mobility.sort_values('date')\n",
    "\n",
    "    mobility['date'] = pd.to_datetime(mobility.date, format='%Y-%m-%d')\n",
    "    mobility['month'] = mobility['date'].to_numpy().astype('datetime64[M]')\n",
    "    mobility = mobility.groupby(['month']).mean().reset_index()\n",
    "\n",
    "    pre_covid_end_month = mobility.month.min() - relativedelta(months=1)\n",
    "    post_covid_start_month = mobility.month.max() + relativedelta(months=1)\n",
    "    future_time_series_start_month = pd.to_datetime(current_month, format = '%Y-%m-%d') + relativedelta(months=1)\n",
    "    end_month = pd.to_datetime(current_month, format = '%Y-%m-%d') + relativedelta(months=future_prediction_months)\n",
    "\n",
    "    pre_covid_date_range = pd.date_range(start=start_date, end=pre_covid_end_month, freq='MS')\n",
    "    post_covid_date_range = pd.date_range(start=min(post_covid_start_month, future_time_series_start_month), \n",
    "                                          end=end_month, freq='MS')\n",
    "    pre_covid_feat = pd.DataFrame({'month': pre_covid_date_range, \n",
    "                                   mobility_col:[pre_covid_baseline]*len(pre_covid_date_range)})\n",
    "    post_covid_feat = pd.DataFrame({'month': post_covid_date_range, \n",
    "                                    mobility_col:[post_covid_baseline]*len(post_covid_date_range)})\n",
    "    mobility = mobility[mobility.month < future_time_series_start_month].append(\n",
    "        pre_covid_feat).append(post_covid_feat).sort_values('month').reset_index(drop=True)\n",
    "    \n",
    "    return mobility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "22647c0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rev_date</th>\n",
       "      <th>retail_and_recreation_percent_change_from_baseline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>2019-08-01</td>\n",
       "      <td>-3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>2019-09-01</td>\n",
       "      <td>-3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>2019-10-01</td>\n",
       "      <td>-3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>2019-11-01</td>\n",
       "      <td>-3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>2019-12-01</td>\n",
       "      <td>-3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      rev_date  retail_and_recreation_percent_change_from_baseline\n",
       "109 2019-08-01                                               -3.0 \n",
       "110 2019-09-01                                               -3.0 \n",
       "111 2019-10-01                                               -3.0 \n",
       "112 2019-11-01                                               -3.0 \n",
       "113 2019-12-01                                               -3.0 "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covid_mobility_df = generate_mobility_feature(current_month = str(current_month), future_prediction_months = validation_window_length)\n",
    "\n",
    "covid_mobility_df = covid_mobility_df.rename(columns={'month':'rev_date'})\n",
    "covid_mobility_df = covid_mobility_df.groupby('rev_date',as_index=False).agg(retail_and_recreation_percent_change_from_baseline = ('retail_and_recreation_percent_change_from_baseline',sum))\n",
    "covid_mobility_df = covid_mobility_df[covid_mobility_df['rev_date']<=current_month+ pd.DateOffset(months = validation_window_length)]\n",
    "covid_mobility_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fe478b",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fad1fa2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit\n",
    "def processing_engine():    \n",
    "    start_time = time.time()\n",
    "    # Import and transform data to the desired level (WEX_ID-month level or business_program_name-month level)\n",
    "    grouped_id_level = data_transform_to_id_level(mdm,dataset_pd,date_year,date_month,forecast_target_column,granularity_level)\n",
    "\n",
    "    # Account should have atleast 2 years of data to train check\n",
    "    if choice == 1: #For only Forecast\n",
    "        print(1)\n",
    "        grouped_id_level['year_check']  = grouped_id_level.progress_apply(dates_checker_2,current_month=current_month,axis=1)\n",
    "        grouped_id_level = grouped_id_level[grouped_id_level['year_check']==1]\n",
    "#         grouped_id_level=grouped_id_level.head(30)\n",
    "    \n",
    "    if choice == 2: #For Forecast and Compare\n",
    "        print(2)\n",
    "        grouped_id_level['year_check']  = grouped_id_level.progress_apply(dates_checker,current_month=current_month,axis=1)\n",
    "        grouped_id_level = grouped_id_level[grouped_id_level['year_check']==1]\n",
    "        grouped_id_level=grouped_id_level.head(50)\n",
    "\n",
    "    # Missing Value treatment\n",
    "    combined_date_list = []\n",
    "    combined_gallons_list = []\n",
    "    ans = grouped_id_level.progress_apply(padder,\n",
    "                                          combined_date_list=combined_date_list,\n",
    "                                          combined_gallons_list=combined_gallons_list,\n",
    "                                          validation_window_length=validation_window_length,\n",
    "                                          current_month=current_month,axis=1) \n",
    "    grouped_id_level['rev_date'] = combined_date_list\n",
    "    grouped_id_level['gallons_list'] = combined_gallons_list\n",
    "    grouped_id_level['total_records_after_padding'] = grouped_id_level.progress_apply(find_len,axis=1)\n",
    "    grouped_id_level_ind = grouped_id_level.reset_index()\n",
    "    grouped_id_level_ind['state'] = grouped_id_level_ind.progress_apply(state_length_check,axis=1)\n",
    "\n",
    "    \n",
    "    # Adding EFD as a feature\n",
    "    grouped_id_level_ind['REV_EQUIVALENT_FUEL_DAY_FACTOR_efd'],\\\n",
    "    grouped_id_level_ind['REV_EQUIVALENT_FUEL_DAY_FACTOR_efd_ma3m'],grouped_id_level_ind['REV_EQUIVALENT_FUEL_DAY_FACTOR_efd_ma6m']= zip(*grouped_id_level_ind.progress_apply(merge_EFD_feature_file,efd_features_monthly=efd_features_monthly,axis=1))\n",
    "    \n",
    "    # Adding covid mobility as a feature\n",
    "    grouped_id_level_ind['covid_mobility_feature']=grouped_id_level_ind.progress_apply(merge_CMD_feature_file,\n",
    "                                                                                       cmd_features_monthly=covid_mobility_df,\n",
    "                                                                                       axis=1)\n",
    "    \n",
    "    \n",
    "    print(\"--- %s seconds for data preparation---\" % (time.time() - start_time))\n",
    "    # Building models from the multiple options available \n",
    "    start_time = time.time()\n",
    "    \n",
    "    models_trained = []     \n",
    "\n",
    "#     if 'HW' in models:\n",
    "#         print('Running HW')\n",
    "#         rows = [(grouped_id_level_ind.loc[i],\n",
    "#                  current_month,\n",
    "#                  validation_window_length,\n",
    "#                  alpha,\n",
    "#                  beta,\n",
    "#                  gamma,\n",
    "#                  granularity_level) for i in grouped_id_level_ind.index]\n",
    "#         results_hw_list = Parallel(n_jobs=mp.cpu_count())(delayed(create_and_train_data_for_hw)(a,b,c,d,e,f,g) for a,b,c,d,e,f,g in rows)\n",
    "\n",
    "#         results_hw = pd.DataFrame(results_hw_list, columns=[granularity_level, 'mape_hw', 'rmse_hw',\\\n",
    "#                             'agorithm_used_hw', 'rev_date_hw', 'ground_truth_hw', 'predictions_hw'])\n",
    "#         results_hw['agorithm_used_hw'] = 'HW'\n",
    "#         models_trained.append(results_hw)\n",
    "#         del rows\n",
    "#         gc.collect()\n",
    "\n",
    "    if 'XGB' in models and len(external_feature_list)>0: \n",
    "        start_time_xgb = time.time()\n",
    "        print('Running XGB')\n",
    "        rows = [(grouped_id_level_ind.loc[i],\n",
    "                current_month,\n",
    "                validation_window_length,\n",
    "                external_feature_list,\n",
    "                learning_rate_xgb,\n",
    "                n_estimators_xgb,\n",
    "                subsample_xgb,\n",
    "                max_depth_xgb,\n",
    "                colsample_bytree_xgb,\n",
    "                min_child_weight_xgb,\n",
    "                granularity_level) for i in grouped_id_level_ind.index]\n",
    "        results_xgb_list = Parallel(n_jobs=mp.cpu_count())(delayed(create_and_train_data_for_xgboost)(a,b,c,d,e,f,g,h,i,j,k) for a,b,c,d,e,f,g,h,i,j,k in rows)\n",
    "\n",
    "        results_xgb = pd.DataFrame(results_xgb_list, columns=[granularity_level, 'mape_xgb', 'rmse_xgb', \n",
    "                            'agorithm_used_xgb', 'rev_date_xgb', 'ground_truth_xgb', 'predictions_xgb'])\n",
    "        results_xgb['agorithm_used_xgb'] = 'XGB'\n",
    "        models_trained.append(results_xgb)\n",
    "        del rows\n",
    "        gc.collect()  \n",
    "        print(\"--- %s seconds for running XGB---\" % (time.time() - start_time_xgb))\n",
    "#     if 'ARIMAX' in models and len(external_feature_list)>0:\n",
    "#         print('Running ARIMAX')\n",
    "#         rows = [(grouped_id_level_ind.loc[i], \n",
    "#                  current_month,\n",
    "#                  validation_window_length,\n",
    "#                  information_criterion,\n",
    "#                  seasonal_arimax,\n",
    "#                  external_feature_list,\n",
    "#                  n_jobs,\n",
    "#                  granularity_level) for i in grouped_id_level_ind.index]\n",
    "#         results_arimax_list = Parallel(n_jobs=mp.cpu_count())(delayed(create_and_train_data_for_arimax)(a,b,c,d,e,f,g,h) for a,b,c,d,e,f,g,h in rows)\n",
    "#         results_arimax = pd.DataFrame(results_arimax_list, columns=[\n",
    "#             granularity_level, 'mape_arimax', 'rmse_arimax', 'agorithm_used_arimax',\n",
    "#             'rev_date_arimax', 'ground_truth_arimax', 'predictions_arimax'\n",
    "#         ])\n",
    "#         results_arimax['agorithm_used_arimax'] = 'ARIMAX'\n",
    "#         models_trained.append(results_arimax)\n",
    "#         del rows\n",
    "#         gc.collect()\n",
    "\n",
    "\n",
    "#     if 'LSTM_Multivariate' in models and len(external_feature_list)>0:\n",
    "#         print('Running LSTM_Multivariate')\n",
    "#         rows = [(grouped_id_level_ind.loc[i],\n",
    "#                  current_month,\n",
    "#                  validation_window_length,\n",
    "#                  external_feature_list,\n",
    "#                  train_period_lstm_model,\n",
    "#                  pred_period_lstm_model,\n",
    "#                  optimizer_lstm_model,\n",
    "#                  loss_function_lstm_model,\n",
    "#                  batch_size_lstm_model,\n",
    "#                  num_epochs_lstm_model,\n",
    "#                 granularity_level) for i in grouped_id_level_ind.index]\n",
    "#         results_lstm_list = Parallel(n_jobs=mp.cpu_count())(delayed(create_and_train_data_for_lstm_multivariate)(a,b,c,d,e,f,g,h,i,j,k) for a,b,c,d,e,f,g,h,i,j,k in rows)\n",
    "        \n",
    "#         results_lstm = pd.DataFrame(results_lstm_list, columns=[\n",
    "#             granularity_level, 'mape_lstm_multivariate', 'rmse_lstm_multivariate', \n",
    "#             'agorithm_used_lstm_multivariate',\n",
    "#             'rev_date_lstm_multivariate', 'ground_truth_lstm_multivariate', \n",
    "#             'predictions_lstm_multivariate'\n",
    "#         ])\n",
    "#         results_lstm['agorithm_used_lstm_multivariate'] = 'LSTM_Multivariate'\n",
    "#         models_trained.append(results_lstm)\n",
    "#         del rows\n",
    "#         gc.collect()\n",
    " \n",
    "\n",
    "    print(\"--- %s seconds for building models---\" % (time.time() - start_time))\n",
    "        \n",
    "    ## COMPARE MODELS\n",
    "    print('Initialising Compare models')\n",
    "    start_time = time.time()\n",
    "    id_best_algo_df = compare_models(models_trained,granularity_level)\n",
    "    grouped_id_level_ind = grouped_id_level_ind.merge(id_best_algo_df,on=granularity_level,how='left')\n",
    "    print(\"--- %s seconds for comparing models---\" % (time.time() - start_time))\n",
    "    \n",
    "    ## CHOOSE and FORECAST using the best model\n",
    "    print('Initialising Forecasting ')\n",
    "    start_time = time.time()\n",
    "    \n",
    "    rows = [(grouped_id_level_ind.loc[i],\n",
    "            current_month,\n",
    "            validation_window_length,\n",
    "            information_criterion,\n",
    "            seasonal_arimax,\n",
    "            external_feature_list,\n",
    "            n_jobs,\n",
    "            learning_rate_xgb,\n",
    "            n_estimators_xgb,\n",
    "            subsample_xgb,\n",
    "            max_depth_xgb,\n",
    "            colsample_bytree_xgb,\n",
    "            min_child_weight_xgb,\n",
    "            train_period_lstm_model,\n",
    "            pred_period_lstm_model,\n",
    "            optimizer_lstm_model,\n",
    "            loss_function_lstm_model,\n",
    "            batch_size_lstm_model,\n",
    "            num_epochs_lstm_model,\n",
    "            alpha,\n",
    "            beta,\n",
    "            gamma,\n",
    "            granularity_level) for i in grouped_id_level_ind.index]\n",
    "    final_res_list = Parallel(n_jobs=mp.cpu_count())(delayed(choose_model_and_forecast)(a,b,c,d,e,f,g,h,i,j,k,l,m,n,o,p,q,r,s,t,u,v,w) for a,b,c,d,e,f,g,h,i,j,k,l,m,n,o,p,q,r,s,t,u,v,w in rows)\n",
    "    \n",
    "    final_res = pd.DataFrame(final_res_list, columns=[granularity_level,\n",
    "                                                     'overall_mape(performance_assessment_period)',\n",
    "                                                     'overall_rmse(performance_assessment_period)',\n",
    "                                                     'model_chosen',\n",
    "                                                     'month',\n",
    "                                                     'gallons_actual(performance_assessment_period)',\n",
    "                                                     'gallons_forecasts(performance_assessment_period)'])\n",
    "    del rows\n",
    "    gc.collect()\n",
    "    error_file = final_res[final_res['model_chosen']=='ERROR']\n",
    "    final_res = final_res[final_res['model_chosen']!='ERROR']\n",
    "    \n",
    "    #final_res['wex_id'],final_res['performance_assesment_mape'],final_res['performance_assesment_rmse'],final_res['agorithm_used'],final_res['performance_assesment_rev_date'],final_res['performance_assesment_ground_truth'],final_res['performance_assesment_forecasts'],final_res['24_months_forecasts']= zip(*grouped_wex_id_level_ind.progress_apply(choose_model_and_forecast,future_prediction_months=future_prediction_months,current_month=current_month,performance_assessment_window=performance_assessment_window,trend=trend,seasonal_hw=seasonal_hw,seasonal_periods=seasonal_period,information_criterion=information_criterion,seasonal_arimax=seasonal_arimax,external_feature_list=external_feature_list,n_jobs=n_jobs,learning_rate_xgb=learning_rate_xgb,n_estimators_xgb=n_estimators_xgb,subsample_xgb=subsample_xgb,max_depth_xgb=max_depth_xgb,colsample_bytree_xgb=colsample_bytree_xgb,min_child_weight_xgb=min_child_weight_xgb,axis=1))\n",
    "    print(\"--- %s seconds for forecasting using best model---\" % (time.time() - start_time))\n",
    "    \n",
    "    ## Output the required files\n",
    "    start_time = time.time()\n",
    "    final_res['3_month_mape(performance_assesment_period)'] = final_res.progress_apply(three_months_mape_calc,axis=1)\n",
    "    final_res['time_weighted_mape(performance_assesment_period)'] = final_res.progress_apply(weighted_mape_calc,axis=1)\n",
    "\n",
    "    final_res['overall_mape(performance_assessment_period)'] = final_res['overall_mape(performance_assessment_period)'].astype('float64')\n",
    "    final_res['time_weighted_mape(performance_assesment_period)'] = final_res['time_weighted_mape(performance_assesment_period)'].astype('float64')\n",
    "    final_res['3_month_mape(performance_assesment_period)'] = final_res['3_month_mape(performance_assesment_period)'].astype('float64')\n",
    "    \n",
    "    final_res['overall_mape(performance_assessment_period)'] = round(final_res['overall_mape(performance_assessment_period)'],2)\n",
    "    final_res['time_weighted_mape(performance_assesment_period)'] = round(final_res['time_weighted_mape(performance_assesment_period)'],2)\n",
    "    final_res['3_month_mape(performance_assesment_period)'] = round(final_res['3_month_mape(performance_assesment_period)'],2)\n",
    "    df_dict = dict()\n",
    "    for idx,dfs in enumerate(models_trained):\n",
    "        model = dfs.iloc[0,3]\n",
    "        df_dict[model] = idx\n",
    "    \n",
    "    final_res['overall_mape(evaluation_period)'],final_res['3_month_mape(evaluation_period)'],\\\n",
    "    final_res['time_weighted_mape(evaluation_period)'],final_res['monthly_avg_volume(evaluation_period)'] =\\\n",
    "    zip(*final_res.progress_apply(records_maintainer,\n",
    "                                  df_dict=df_dict,models_trained=models_trained,\n",
    "                                  granularity_level=granularity_level,axis=1))\n",
    "    \n",
    "    metrics_file = final_res[[granularity_level,'model_chosen','monthly_avg_volume(evaluation_period)','overall_mape(evaluation_period)',\n",
    "                             '3_month_mape(evaluation_period)','time_weighted_mape(evaluation_period)',\n",
    "                             'overall_mape(performance_assessment_period)','3_month_mape(performance_assesment_period)',\n",
    "                             'time_weighted_mape(performance_assesment_period)']]\n",
    "    \n",
    "    \n",
    "    final_res = final_res.merge(grouped_id_level_ind[[granularity_level,'rev_date','gallons_list']], on =granularity_level,how='left')   \n",
    "    metric_file = pd.DataFrame()\n",
    "    metric_file['data'] = final_res.progress_apply(metric_file_generator,\n",
    "                                                   df_dict=df_dict, models_trained=models_trained,models=models,\n",
    "                                                   granularity_level=granularity_level,axis=1) \n",
    "    total_forecast_file = pd.DataFrame()\n",
    "    for i in range(len(metric_file)):\n",
    "        total_forecast_file = total_forecast_file.append(metric_file.iloc[i][0],ignore_index=True)\n",
    "        \n",
    "    ##Exception accounts\n",
    "    print('Handling Exception accounts')\n",
    "    final_res_B = final_res[final_res['overall_mape(evaluation_period)']>=40]\n",
    "    final_res11gt40_wids = list(final_res_B[granularity_level])\n",
    "\n",
    "    grouped_id_level = data_transform_to_id_level(mdm,dataset_pd,date_year,date_month,forecast_target_column,granularity_level)\n",
    "    \n",
    "    if choice == 1:\n",
    "        print(1)\n",
    "        grouped_id_level['year_check']  = grouped_id_level.progress_apply(dates_checker_2,current_month=current_month,axis=1)\n",
    "    if choice == 2:\n",
    "        print(2)\n",
    "        grouped_id_level['year_check']  = grouped_id_level.progress_apply(dates_checker,current_month=current_month,axis=1)\n",
    "\n",
    "    grouped_id_level_ind_A = grouped_id_level.reset_index()\n",
    "\n",
    "    grouped_id_level_ind_A['ev_mape_gt40_check']  = np.where((grouped_id_level_ind_A[granularity_level].isin(final_res11gt40_wids)),1,0)\n",
    "    grouped_id_level = grouped_id_level_ind_A.set_index(granularity_level)\n",
    "\n",
    "    grouped_id_level_AB = grouped_id_level[(grouped_id_level['year_check']==2) | (grouped_id_level['ev_mape_gt40_check']==1)]\n",
    "\n",
    "    #Padding\n",
    "    combined_date_list_AB = []\n",
    "    combined_gallons_list_AB = []\n",
    "    ans_AB = grouped_id_level_AB.progress_apply(padder,\n",
    "                                                combined_date_list=combined_date_list_AB,\n",
    "                                                combined_gallons_list=combined_gallons_list_AB,\n",
    "                                                validation_window_length=validation_window_length,\n",
    "                                                current_month=current_month,axis=1) \n",
    "    grouped_id_level_AB['rev_date'] = combined_date_list_AB\n",
    "    grouped_id_level_AB['gallons_list'] = combined_gallons_list_AB\n",
    "    grouped_id_level_AB['total_records_after_padding'] = grouped_id_level_AB.progress_apply(find_len,axis=1)\n",
    "    grouped_id_level_ind_AB = grouped_id_level_AB.reset_index()\n",
    "    grouped_id_level_ind_AB['state'] = grouped_id_level_ind_AB.progress_apply(state_length_check,axis=1)    \n",
    "    grouped_id_level_ind_AB = grouped_id_level_ind_AB[grouped_id_level_ind_AB['total_records_after_padding']>=18]\n",
    "    \n",
    "    ##storing ids\n",
    "    smallaccounts_ids = list(grouped_id_level_ind_AB[grouped_id_level_ind_AB['year_check']==2][granularity_level])\n",
    "\n",
    "    grouped_id_level_ind_AB['REV_EQUIVALENT_FUEL_DAY_FACTOR_efd'],\\\n",
    "    grouped_id_level_ind_AB['REV_EQUIVALENT_FUEL_DAY_FACTOR_efd_ma3m'],\\\n",
    "    grouped_id_level_ind_AB['REV_EQUIVALENT_FUEL_DAY_FACTOR_efd_ma6m']=\\\n",
    "    zip(*grouped_id_level_ind_AB.progress_apply(merge_EFD_feature_file,efd_features_monthly=efd_features_monthly,axis=1))\n",
    "\n",
    "    # Adding covid mobility as a feature\n",
    "    grouped_id_level_ind_AB['covid_mobility_feature']=grouped_id_level_ind_AB.progress_apply(merge_CMD_feature_file,\n",
    "                                                                                       cmd_features_monthly=covid_mobility_df,\n",
    "                                                                                       axis=1)\n",
    "    \n",
    "    \n",
    "    ## Modelling for Exception accounts\n",
    "    grouped_id_level_ind_AB = grouped_id_level_ind_AB.sample(10)\n",
    "    #ARIMAX\n",
    "    grouped_id_level_ind_AB = grouped_id_level_ind_AB.assign(best_algo='ARIMAX')   #model_for_exception_accounts \n",
    "    rows = [(grouped_id_level_ind_AB.loc[i],\n",
    "            current_month,\n",
    "            validation_window_length,\n",
    "            information_criterion,\n",
    "            seasonal_arimax,\n",
    "            external_feature_list,\n",
    "            n_jobs,\n",
    "            learning_rate_xgb,\n",
    "            n_estimators_xgb,\n",
    "            subsample_xgb,\n",
    "            max_depth_xgb,\n",
    "            colsample_bytree_xgb,\n",
    "            min_child_weight_xgb,\n",
    "            train_period_lstm_model,\n",
    "            pred_period_lstm_model,\n",
    "            optimizer_lstm_model,\n",
    "            loss_function_lstm_model,\n",
    "            batch_size_lstm_model,\n",
    "            num_epochs_lstm_model,\n",
    "            alpha,\n",
    "            beta,\n",
    "            gamma,\n",
    "            granularity_level) for i in grouped_id_level_ind_AB.index]\n",
    "    final_res_exp_list_1 = Parallel(n_jobs=mp.cpu_count())(delayed(choose_model_and_forecast)(a,b,c,d,e,f,g,h,i,j,k,l,m,n,o,p,q,r,s,t,u,v,w) for a,b,c,d,e,f,g,h,i,j,k,l,m,n,o,p,q,r,s,t,u,v,w in rows)\n",
    "\n",
    "    final_res_exp_1 = pd.DataFrame(final_res_exp_list_1, columns=[granularity_level,\n",
    "                                                     'overall_mape(performance_assessment_period)',\n",
    "                                                     'overall_rmse(performance_assessment_period)',\n",
    "                                                     'model_chosen',\n",
    "                                                     'month',\n",
    "                                                     'gallons_actual(performance_assessment_period)',\n",
    "                                                     'gallons_forecasts(performance_assessment_period)'])\n",
    "    del rows\n",
    "    gc.collect()   \n",
    "    \n",
    "    #LSTM_MULTIVARIATE\n",
    "    grouped_id_level_ind_AB = grouped_id_level_ind_AB.assign(best_algo='LSTM_MULTIVARIATE')   #model_for_exception_accounts \n",
    "    train_period_lstm_model_exp = 5\n",
    "    rows = [(grouped_id_level_ind_AB.loc[i],\n",
    "            current_month,\n",
    "            validation_window_length,\n",
    "            information_criterion,\n",
    "            seasonal_arimax,\n",
    "            external_feature_list,\n",
    "            n_jobs,\n",
    "            learning_rate_xgb,\n",
    "            n_estimators_xgb,\n",
    "            subsample_xgb,\n",
    "            max_depth_xgb,\n",
    "            colsample_bytree_xgb,\n",
    "            min_child_weight_xgb,\n",
    "            train_period_lstm_model_exp,\n",
    "            pred_period_lstm_model,\n",
    "            optimizer_lstm_model,\n",
    "            loss_function_lstm_model,\n",
    "            batch_size_lstm_model,\n",
    "            num_epochs_lstm_model,\n",
    "            alpha,\n",
    "            beta,\n",
    "            gamma,\n",
    "            granularity_level) for i in grouped_id_level_ind_AB.index]\n",
    "    final_res_exp_list_2 = Parallel(n_jobs=mp.cpu_count())(delayed(choose_model_and_forecast)(a,b,c,d,e,f,g,h,i,j,k,l,m,n,o,p,q,r,s,t,u,v,w) for a,b,c,d,e,f,g,h,i,j,k,l,m,n,o,p,q,r,s,t,u,v,w in rows)\n",
    "\n",
    "    final_res_exp_2 = pd.DataFrame(final_res_exp_list_2, columns=[granularity_level,\n",
    "                                                     'overall_mape(performance_assessment_period)',\n",
    "                                                     'overall_rmse(performance_assessment_period)',\n",
    "                                                     'model_chosen',\n",
    "                                                     'month',\n",
    "                                                     'gallons_actual(performance_assessment_period)',\n",
    "                                                     'gallons_forecasts(performance_assessment_period)'])\n",
    "    del rows\n",
    "    gc.collect()\n",
    "    \n",
    "    #ENSEMBLER\n",
    "    final_res_exp = pd.DataFrame()\n",
    "    gallons_forecasts = []\n",
    "\n",
    "    for i in range(final_res_exp_2.shape[0]):\n",
    "        gallons_forecasts.append(mean_calc(final_res_exp_1['gallons_forecasts(performance_assessment_period)'].iloc[i],final_res_exp_2['gallons_forecasts(performance_assessment_period)'].iloc[i]))\n",
    "    final_res_exp[granularity_level] = final_res_exp_2[granularity_level]\n",
    "    final_res_exp['gallons_actual(performance_assessment_period)'] = final_res_exp_2['gallons_actual(performance_assessment_period)']\n",
    "    final_res_exp['gallons_forecasts(performance_assessment_period)'] = gallons_forecasts\n",
    "    final_res_exp['model_chosen'] = 'ENSEMBLE(LSTM_and_ARIMAX)'\n",
    "    final_res_exp['month'] = final_res_exp_2['month']\n",
    "    final_res_exp['overall_mape(performance_assessment_period)'] = final_res_exp.progress_apply(twelve_months_mape_calc,axis=1)\n",
    "    final_res_exp['overall_rmse(performance_assessment_period)'] = final_res_exp.progress_apply(twelve_months_rmse_calc,axis=1)\n",
    "\n",
    "    error_file_exp = final_res_exp[final_res_exp['model_chosen']=='ERROR']\n",
    "    final_res_exp = final_res_exp[final_res_exp['model_chosen']!='ERROR']\n",
    "\n",
    "    final_res_exp['3_month_mape(performance_assesment_period)'] = final_res_exp.progress_apply(three_months_mape_calc,axis=1)\n",
    "    final_res_exp['time_weighted_mape(performance_assesment_period)'] = final_res_exp.progress_apply(weighted_mape_calc,axis=1)\n",
    "\n",
    "    final_res_exp = final_res_exp.merge(grouped_id_level_ind_AB[[granularity_level,'rev_date','gallons_list']],on=granularity_level,how='left')\n",
    "    metrics_fileAB = final_res_exp[[granularity_level,'model_chosen','overall_mape(performance_assessment_period)',\n",
    "                              '3_month_mape(performance_assesment_period)','time_weighted_mape(performance_assesment_period)']]\n",
    "    \n",
    "    metric_fileAB = pd.DataFrame()\n",
    "    metric_fileAB['data'] = final_res_exp.progress_apply(metric_file_generator_for_exception_accounts,granularity_level=granularity_level,axis=1) \n",
    "    total_forecast_fileAB = pd.DataFrame()\n",
    "    for i in range(len(metric_fileAB)):\n",
    "        total_forecast_fileAB = total_forecast_fileAB.append(metric_fileAB.iloc[i][0],ignore_index=True)\n",
    "    \n",
    "    exception_ids = list(final_res_exp[granularity_level])   \n",
    "    metrics_file = metrics_file[~(metrics_file[granularity_level].isin(exception_ids))]\n",
    "    total_forecast_file = total_forecast_file[~(total_forecast_file[granularity_level].isin(exception_ids))]\n",
    "    metrics_file = metrics_file[~(metrics_file[granularity_level].isin(exception_ids))]\n",
    "    final_res = final_res[~(final_res[granularity_level].isin(exception_ids))]\n",
    "    \n",
    "    total_forecast_file = total_forecast_file.append(total_forecast_fileAB).reset_index(drop=True)\n",
    "    metrics_file = metrics_file.append(metrics_fileAB).reset_index(drop=True)\n",
    "    final_res = final_res.append(final_res_exp).reset_index(drop=True)\n",
    "    \n",
    "    ## Putting status  flags to differentiate different accounts\n",
    "    conditions = [final_res[granularity_level].isin(final_res11gt40_wids), \n",
    "                  final_res[granularity_level].isin(smallaccounts_ids)]\n",
    "    choices =    ['accounts with greater than 40 evaluation period MAPE', \n",
    "                  'accounts with less than 36 datapoints']\n",
    "    final_res['status_flag'] = np.select(conditions, choices, default='eligible accounts')\n",
    "    \n",
    "    conditions = [total_forecast_file[granularity_level].isin(final_res11gt40_wids), \n",
    "                  total_forecast_file[granularity_level].isin(smallaccounts_ids)]\n",
    "    choices =    ['accounts with greater than 40 evaluation period MAPE', \n",
    "                  'accounts with less than 36 datapoints']\n",
    "    total_forecast_file['status_flag'] = np.select(conditions, choices, default='eligible accounts')\n",
    "    \n",
    "    conditions = [metrics_file[granularity_level].isin(final_res11gt40_wids), \n",
    "                  metrics_file[granularity_level].isin(smallaccounts_ids)]\n",
    "    choices =    ['accounts with greater than 40 evaluation period MAPE', \n",
    "                  'accounts with less than 36 datapoints']\n",
    "    metrics_file['status_flag'] = np.select(conditions, choices, default='eligible accounts')\n",
    "    \n",
    "    if choice == 1:\n",
    "        print(1)\n",
    "        metrics_file.drop(['overall_mape(performance_assessment_period)',\n",
    "                           '3_month_mape(performance_assesment_period)',\n",
    "                           'time_weighted_mape(performance_assesment_period)'],axis=1,inplace=True)\n",
    "\n",
    "    metrics_file.to_csv('metrics_file.csv',index=False)\n",
    "    total_forecast_file.to_csv('forecasts.csv',index=False)\n",
    "    final_res.to_csv('final_results.csv',index=False)\n",
    "    print(\"--- %s seconds for outputting the required files---\" % (time.time() - start_time))\n",
    "\n",
    "\n",
    "    return models_trained,grouped_id_level_ind,id_best_algo_df,final_res,total_forecast_file,metrics_file,error_file,error_file_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "192ca7cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f42a15b10c1d4dfebc992b2acdf4ac77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3812 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb41fd22a2c7447da7adbf94c4afd649",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3812 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4e13f61806343049900143300db4c70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "650c1b307c7a44efb58e29c8e3196c1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c72e852669b547048ee119b8a1c05780",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a63fa48460ce488a8a18370dace11180",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e08b319ad3714e58b9d31941368fea52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cee743e38c74e17814cacaf069479ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "852aa3b923cf44028845a844ff06ce9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b242074077414ee998013b69af7521bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "492eac0e2a3e47c9b140db0da8ee1eb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25517ede1e32449c8b6e7e049449d38c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3812 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f6aa303df2041e0b5cda9b5401d1eb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3812 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c73bc99f2d645a2b4e18d2941a63936",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/701 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8500e3c098e4522a7c3425fe6b9697b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/701 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffae4a0e5f3c4cbf986a4ebad5b1dd58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/701 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6dfb697a6ab486a9adcc5558686e6f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/623 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2747d715fdd846949b67ae1e2dacfeff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/623 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13a36fde35304bda889be2fba0627996",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9df89b224d174ac0a298a7d455f9083a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13487218e2fb488a9142f6fcbaabe7b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49177f263c6143ef9b31df63979354ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dcdd8f956fe47fc8bf5ce42e0f2a2b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "old_stdout = sys.stdout\n",
    "log_file = open(\"message.log\",\"w\")\n",
    "sys.stdout = log_file\n",
    "models_trained,grouped_id_level_ind,id_best_algo_df,final_res,\\\n",
    "total_forecast_file,metrics_file,error_file,error_file_exp= processing_engine()\n",
    "sys.stdout = old_stdout\n",
    "log_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f0c2935a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGB                          49\n",
       "ENSEMBLE(LSTM_and_ARIMAX)    10\n",
       "Name: model_chosen, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_res['model_chosen'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfdd675",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
